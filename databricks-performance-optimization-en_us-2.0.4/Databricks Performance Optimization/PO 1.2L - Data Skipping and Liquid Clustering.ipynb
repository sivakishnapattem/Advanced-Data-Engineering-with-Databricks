{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "108b5154-8e12-47b6-af02-a7104d6f9851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d02e8b8-e464-4377-8b02-c9fadcdd080c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab: Data Skipping and Liquid Clustering\n",
    "\n",
    "In this demo, we are going to work with Liquid Clustering, a Delta Lake optimization feature that replaces table partitioning and ZORDER to simplify data layout decisions and optimize query performance. It provides flexibility to redefine clustering keys without rewriting data. Refer to the [documentation](https://docs.databricks.com/en/delta/clustering.html) for more information.\n",
    "\n",
    "#### Learning Objectives\n",
    "**By the end of this lab, you will be able to:**\n",
    "\n",
    "* Disable Spark caching to observe the effects of Liquid Clustering.\n",
    "* Count records and explore data in the flights tables.\n",
    "* Execute queries on an unclustered table and analyze their performance using Spark UI.\n",
    "* Execute and compare queries on tables clustered by different columns (**id** and **id** + **FlightNum**).\n",
    "* Inspect query performance using the Spark UI to understand the benefits of Liquid Clustering.\n",
    "\n",
    "#### Prerequisites\n",
    "In order to follow along with this lab, you will need:\n",
    "\n",
    "* Basic knowledge of running SQL queries in Databricks is required.\n",
    "* Familiarity with Delta Lake and its optimization features is recommended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6448175-309c-4fe8-bc18-0b91ce858aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default. If you use Serverless, errors will be returned when setting compute runtime properties.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc84e373-8b2f-440b-af9d-f1b0ddf28968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to your unique **labuser** catalog, and the default schema to **default**. All tables will be read from and written to this location.\n",
    "<br></br>\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a37832dc-6186-4616-96a9-4b91cfc25fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-2L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fd28221-cc57-40f3-a1b2-db30cf8e978f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Set the default catalog to **dbacademy_flightdata** and the schema to **v01**. We will use the read-only tables in this location for the demonstration.\n",
    "\n",
    "**NOTE:** These tables are shared through the Databricks Marketplace, provided by **Databricks**. The name of the share is **Airline Performance Data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4366ae7-475e-410d-ab77-a99d9622678d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USE CATALOG dbacademy_flightdata;\n",
    "USE SCHEMA v01;\n",
    "\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f35f43f3-beac-487e-a223-ad529f16b725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Disable Caching\n",
    "\n",
    "Run the following cell to set a Spark configuration variable that disables disk caching.\n",
    "\n",
    "Turning disk caching off prevents Databricks from storing cloud storage files after the first query. This makes the effect of the optimizations more apparent by ensuring that files are always pulled from cloud storage for each query.\n",
    "\n",
    "For more information, see [Optimize performance with caching on Databricks](https://docs.databricks.com/en/optimizations/disk-cache.html#optimize-performance-with-caching-on-databricks).\n",
    "\n",
    "**NOTE:** This will not work in Serverless. Please use classic compute to turn off caching. If you're using Serverless, an error will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c59c15d-d3c3-4ac1-88e5-6065d2736de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Set the spark configuration variable \"io.cache\" as \"False\"\n",
    "spark.conf.set('spark.databricks.io.cache.enabled', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "211b3bae-adac-4b7f-9e5e-a696df7afcc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Inspecting the tables with and without clustering, using Spark UI to identify that data is skipped at the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8330d7c-0564-4651-9e39-3964330bcf0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Explore the Data\n",
    "In this lab, we will be using airline flight data that has been saved in three different ways:\n",
    "- **flights**: OPTIMIZED with a ZORDER on **FlightNum**.\n",
    "- **flights_cluster_id**: Liquid clustered using the **id** column.\n",
    "- **flights_cluster_id_flightnum**: Liquid clustered by two columns (**id** and **FlightNum**).\n",
    "\n",
    "Each table contains the exact same data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c13d9ec1-6c45-4b71-ac14-9272f63a5bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Preview the **flights** table. Notice that it contains the following columns: **id, year, FlightNum, ArrDelay, UniqueCarrier, TailNum**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "842e978e-f448-4f51-9b0f-fa9def066f25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM flights\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "402b86ba-f5a8-4e79-910f-aad24403526b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Count the number of rows in each of the flight tables (**flights, flights_cluster_id, flights_cluster_id_flightnum**) to confirm that the number of rows in each table is *1,235,347,780*.\n",
    "\n",
    "    Each table contains the same data but is stored differently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe1ed942-7fe1-4b5b-8959-be88ddfd7ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT 'flights', count(*) AS TotalRows\n",
    "FROM flights\n",
    "UNION ALL\n",
    "SELECT 'flights_cluster_id', count(*) AS TotalRows\n",
    "FROM flights_cluster_id\n",
    "UNION ALL\n",
    "SELECT 'flights_cluster_id_flightnum', count(*) AS TotalRows\n",
    "FROM flights_cluster_id_flightnum;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09f73ae5-46cf-43ad-aeab-e4381e031498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Query Performance with ZORDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7de478f6-b2cf-4e22-9df2-d7aa6d663464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Execute the cell below to view the history of the **flights** table. In the output, observe the following:\n",
    "    - In the **operation** column, find the version (row) of the Delta table that has been *OPTIMIZED*.\n",
    "\n",
    "    - In the **operationParameters** column, notice that the **flights** table has been optimized with a ZORDER on the **FlightNum** column. Z-ordering is a technique used to colocate related information in the same set of files.\n",
    "\n",
    "    - In the **operationMetrics** column, observe that the OPTIMIZED statement removed (*numRemovedFiles*) *160* files and added (*numAddedFiles*) *31* files to optimize the storage of the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86695e5f-2ea6-40ce-994a-d0baeab938b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY flights;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7191204-7ed3-490a-9577-2ef22b95c797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C2.1 Unique Carrier Column Query\n",
    "1. Run the following query to analyze the average arrival delay in flights by querying the **UniqueCarrier** column for the *TW* airline. Take note of the time it took for the query to execute.\n",
    "\n",
    "**NOTE:** The **flights** table contains a ZORDER on **FlightNum**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffa5db82-dac1-4efe-b632-d6d957b0bcc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Perform the SELECT operation with the AVG operation on the 'ArrDelay' column and the 'UniqueCarrier' set to 'TW'.\n",
    "\n",
    "SELECT AVG(try_cast(ArrDelay AS DOUBLE))\n",
    "FROM flights\n",
    "WHERE UniqueCarrier = 'TW';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5319ba61-d1ae-474a-b2ac-2f1d95a18ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's see how this query performed using the Spark UI. Note in particular the amount of cloud storage requests and their associated time. To view how the query performed complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **SQL/DataFrame Properties** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotonScan parquet dbacademy_flightdata.v01.flights (1)** and select the plus icon.\n",
    "\n",
    "\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary slightly):\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count total (min, med, max)| 149 (3, 3, 18)|  Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data.<br></br>Monitoring this metric helps optimize performance, reduce costs, and identify potential inefficiencies in data access patterns.|\n",
    "| cloud storage response size total (min, med, max)|1068.4 MiB (336.6 KiB, 45.7 MiB, 50.7 MiB)| Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.<br></br> The data transferred ranged from small to large requests, with an average response size around 45.7 MiB.|\n",
    "| files pruned | 0 |A total of 0 files were skipped by Spark due to pruning based on the query's filters. This is due to the table being z-ordered by **FlightNum** but queried by the **UniqueCarrier** column.|\n",
    "| files read | 31| All 31 files were read during the execution of the Spark job.|\n",
    "\n",
    "\n",
    "#### Summary\n",
    "This table was Z-ordered by **FlightNum** but queried by the **UniqueCarrier** column. Spark needs to read all of the files to filter the table and return a single row. On average, this query will take about ~10 seconds to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "993f986d-f684-4d95-afb5-1fe96a00b73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C2.2 FlightNum Column (ZORDER column) Query\n",
    "1. Run the following query to analyze the average arrival delay by flight number (**FlightNum**) *1809*. Take note of the time it took for the query to execute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd59a8df-1407-40fd-9d30-70d94e2219a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Perform the SELECT operation with the AVG operation on the 'ArrDelay' column and the 'FlightNum' set to '1890'.\n",
    "SELECT AVG(try_cast(ArrDelay AS DOUBLE)) \n",
    "FROM flights\n",
    "WHERE FlightNum = 1890"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8042dbd-8165-4d5b-957b-9ff47e8785f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Note in particular the amount of cloud storage requests and their associated time. To view how the query performed complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **SQL/DataFrame Properties** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotonScan parquet dbacademy_flightdata.v01.flights (1)** and select the plus icon.\n",
    "\n",
    "\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary slightly):\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count total (min, med, max)| 6 (3, 3, 3)|  Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data.|\n",
    "| cloud storage response size total (min, med, max)|53.5 MiB (26.3 MiB, 27.2 MiB, 27.2 MiB)| Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.<br></br> The min, med and max suggest that the cloud storage requests were relatively consistent in size.|\n",
    "| files pruned | 30 |A total of 30 files were skipped by Spark due to pruning based on the query's filters. This is due to the table being z-ordered by **FlightNum** and queried by the **FlightNum** column.|\n",
    "| files read | 1| Only 1 file was read during the execution of the Spark job.|\n",
    "\n",
    "\n",
    "#### Summary\n",
    "This table was Z-ordered by **FlightNum** and queried by the **FlightNum** column. In this scenario, the files were organized by **FlightNum** and queried by **FlightNum**, so it was optimized for the query to efficiently read the necessary files. On average, this query will take about ~2 seconds to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9147f836-ccbc-4ed2-a82a-c80ed9f26329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C2.3 id Column Query\n",
    "1. Run the following query to analyze the records in the flight table with the **id** *1125281431554*. Take note of the time it took for the query to execute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a60b21f1-5e41-4e55-8e51-4459c12ff4c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM flights\n",
    "WHERE id = 1125281431554;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97c88d49-6ec2-4a11-86b4-6a0be4c30b81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Note in particular the amount of cloud storage requests and their associated time. To view how the query performed complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **SQL/DataFrame Properties*** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotonScan parquet dbacademy_flightdata.v01.flights (1)** and select the plus icon.\n",
    "\n",
    "\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary slightly):\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count total (min, med, max)| 267 (1, 4, 14)| Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data.|\n",
    "| cloud storage response size total (min, med, max) | 4.7 GiB (256.0 KiB, 72.5 MiB, 101.1 MiB) | Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.<br></br> The total data transferred is extremely large at around 4.7 GiB and the individual request sizes vary greatly.|\n",
    "| files pruned | 0 |A total of 0 files were skipped by Spark due to pruning based on the query's filters. This is due to the table being z-ordered by **FlightNum** and queried by the high cardinality **id** column.|\n",
    "| files read | 31| All files were read during the execution of the Spark job.|\n",
    "\n",
    "\n",
    "#### Summary\n",
    "This table was Z-ordered by **FlightNum** and queried by the **id** column. The **id** column is a high-cardinality column, which causes the query to:\n",
    "- Have a very large cloud storage response size.\n",
    "- Read every file.\n",
    "- Run for around ~28 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8efd6cf0-6f56-430b-82de-b5805347db03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C3. Explore Liquid Clustering on Tables\n",
    "Delta Lake liquid clustering replaces table partitioning and ZORDER to simplify data layout decisions and optimize query performance. It provides flexibility to redefine clustering keys without rewriting existing data, allowing data layout to evolve alongside analytic needs over time.  \n",
    "\n",
    "Databricks recommends liquid clustering for all new Delta tables. Scenarios benefiting from clustering include:\n",
    "\n",
    "* Tables often filtered by high cardinality columns.\n",
    "* Tables with a significant skew in data distribution.\n",
    "* Tables that grow quickly and require maintenance and tuning effort.\n",
    "* Tables with concurrent write requirements.\n",
    "* Tables with access patterns that change over time.\n",
    "* Tables where a typical partition key could leave the table with too many or too few partitions.\n",
    "\n",
    "For more information on how to enable the liquid clustering, refer to the [documentation](https://docs.databricks.com/en/delta/clustering.html#enable-liquid-clustering)\n",
    "\n",
    "**NOTE:** These queries on the ZORDERED table are already quite fast without using clustering, considering we are using a small cluster, and the tables are not extremely large. But there is room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "268d43d3-db7c-4e1c-bf64-70b00cefcee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C3.1 Query Performance with Liquid Clustering (id column)\n",
    "Run the following three queries. These queries pull data from the **flights_cluster_id** table. This table is exactly the same as the one we used in the queries above, except that we have enabled liquid clustering by adding `CLUSTER BY (id)` when the table was created.  \n",
    "\n",
    "Note the following:\n",
    "- When we query by the clustered column (**id**), we see an improvement in query performance\n",
    "- We don't see a degradation in performance on queries against unclustered columns  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac1fe467-7b35-4adb-bbdc-adcfacc30ae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Execute the cell below to view the history of the **flights_cluster_id** table. In the output, observe the following:\n",
    "    - In the **operationParameters** column, notice that the **flights_cluster_id** table has been optimized with a *clusterBy* on the **ID** column.\n",
    "    - In the **operationMetrics** column, notice that the clustered table was created with *128* files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9347842-3598-4e82-9488-a214e295235c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY flights_cluster_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5363924-095a-45f8-b9bd-2988be549fc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. View detailed metadata about the **flights_cluster_id** table. Notice the following:\n",
    "    - The table contains the same 6 columns.\n",
    "    - Under *Clustering Information*, we see that the table is clustered by **id**.\n",
    "    - At the bottom of the results, the **Table Properties** contain a variety of properties for liquid clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b95ffa4-6b0d-48f8-8d73-48a86ed90533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE TABLE EXTENDED flights_cluster_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f08020d-4c7a-4f25-8b38-3d986ab973a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### C3.1.1 Unique Carrier Column Query\n",
    "1. Run the following query to analyze the average arrival delay in flights for the *TW* airline. Take note of the time it took for the query to execute.\n",
    "\n",
    "    **NOTE:** The **flights_cluster_id** table is clustered by **id**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61871998-3e6f-4ae6-b537-9f12b65eb794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Perform the SELECT operation on the 'flights_cluster_id' table with the AVG operation on the 'ArrDelay' column and the 'UniqueCarrier' set to 'TW'.\n",
    "SELECT AVG(try_cast(ArrDelay AS DOUBLE)) \n",
    "FROM flights_cluster_id\n",
    "WHERE UniqueCarrier = 'TW';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83d49a14-b5fd-48ba-af3e-1b3873cd18d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Note in particular the amount of cloud storage requests and their associated time. To view how the query performed complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **SQL/DataFrame Properties** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotonScan parquet dbacademy_flightdata.v01.flights_cluster_id (1)** and select the plus icon.\n",
    "\n",
    "\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary slightly):\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count total (min, med, max)| 316 (4, 8, 12)|  Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data.|\n",
    "| cloud storage response size total (min, med, max)|1047.8 MiB (601.0 KiB, 27.5 MiB, 47.2 MiB) | Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.|\n",
    "| files pruned | 0 |A total of 0 files were skipped by Spark due to pruning based on the query's filters.|\n",
    "| files read | 128 | All files were read during the execution of the Spark job.|\n",
    "\n",
    "\n",
    "#### Summary\n",
    "This table was clustered by **id** and queried by the **UniqueCarrier** column, taking about ~10 seconds to execute. The execution time and cloud storage response size are very similar to the query on the optimized **flights** table with a ZORDER on **FlightNum**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d37d7d64-d3c2-4922-a52b-c34a39653603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### C3.1.2 FlightNum Column Query\n",
    "1. Run the following query to analyze the average arrival delay by flight number (**FlightNum**) *1809*. Take note of the time it took for the query to execute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ca7707-db1d-4b93-b8d1-c3619e746cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Perform the SELECT operation on the 'flights_cluster_id' table with the AVG operation on the 'ArrDelay' column and the 'FlightNum' set to '1890'.\n",
    "SELECT AVG(try_cast(ArrDelay AS DOUBLE)) \n",
    "FROM flights_cluster_id\n",
    "WHERE FlightNum = 1890;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08674946-34d7-4dbe-a4ad-5c38e3ec4d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Note in particular the amount of cloud storage requests and their associated time. To view how the query performed complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **SQL/DataFrame Properties** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotonScan parquet dbacademy_flightdata.v01.flights_cluster_id (1)** and select the plus icon.\n",
    "\n",
    "\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary slightly):\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count total (min, med, max)| \t275 (4, 12, 15)|Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data.|\n",
    "| cloud storage response size total (min, med, max)|1871.3 MiB (22.8 MiB, 85.3 MiB, 94.2 MiB) | Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.|\n",
    "| files pruned | 0 |A total of 0 files were skipped by Spark due to pruning based on the query's filters. This is due to the table being clustered by **id** and queried by the **FlightNum** column.|\n",
    "| files read | 128 | All files were read during the execution of the Spark job.|\n",
    "\n",
    "\n",
    "#### Summary\n",
    "This table was clustered by **id** and queried by the **FlightNum** column, taking about ~10 seconds to execute. This query took a bit longer to execute, and the cloud storage response size was much larger than the query on the optimized **flights** table with a ZORDER on **FlightNum**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe6945ff-b6ba-45da-963c-c94490cc1863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### C3.1.3 id Column Query\n",
    "1. Run the following query to analyze the records in the flight table with the **id** `1125281431554`. Take note of the time it took for the query to execute. Remember, the query on the **id** column in the **flights** table took about ~28 seconds to execute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "276eabce-b734-4294-a40e-e1b107dff689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM flights_cluster_id\n",
    "WHERE id = 1125281431554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8f41a86-f2b7-484e-889f-9dcf51c51326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Note in particular the amount of cloud storage requests and their associated time. To view how the query performed complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **SQL/DataFrame Properties** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotonScan parquet dbacademy_flightdata.v01.flights_cluster_id (1)** and select the plus icon.\n",
    "\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary slightly):\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count total | 4| Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data.|\n",
    "| cloud storage response size total |54.5 MiB| Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.|\n",
    "| files pruned | 127 |A total of 127 files were skipped by Spark due to pruning based on the query's filters. This is due to the table being clustered by **id** and queried by the **id** column.|\n",
    "| files read | 1| Only 1 file was read during the execution of the Spark job.|\n",
    "\n",
    "\n",
    "#### Summary\n",
    "This table was clustered by **id** and queried by the **id** column. This enables Spark to optimally read the data and execute in around ~2 seconds, compared to ~28 seconds for the **flights** table, which contained a ZORDER on **FlightNum** and was optimized. The cloud storage response size was also much smaller than 4.7 GiB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "607f8a29-993d-411e-a012-2e52d71f682e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C3.2 Query Performance with Liquid Clustering (id and flight_num columns)\n",
    "Run the following three queries. These queries pull data from the **flights_cluster_id_flightnum** table. This table is clustered by both the **id** and **flight_num** columns.  \n",
    "\n",
    "Note the following:\n",
    "- We still don't have any degradation on unclustered columns. Had we used `PARTITION BY` to partition by **FlightNum** and **id**, we would see massive slowdown for any queries not on those columns, and writes would be prohibitively slow for this volume of data\n",
    "- Now queries on **FlightNum** are improved\n",
    "- Queries are a little slower on **id** now, however and we can look at the DAG to see why.\n",
    "\n",
    "Note that, because  we had to read more files to satisfy this request. There is a (small) cost to clustering on more columns, so choose wisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b27f9d7-3168-4918-83fa-6a62189f43dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Execute the cell below to view the history of the **flights_cluster_id_flightnum** table. In the output, observe the following:\n",
    "    - In the **operationParameters** column, notice that the **flights_cluster_id_flightnum** table has been optimized with a *clusterBy* on the **id** and **FlightNum** columns.\n",
    "    - In the **operationMetrics** column, notice that the clustered table was created with *144* files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb278091-0cc5-4c15-84d3-ba5e6d80d4a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY flights_cluster_id_flightnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b84589-deac-41c3-a626-758fe1f39381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. View detailed metadata about the **flights_cluster_id_flightnum** table. Notice the following:\n",
    "    - Under *Clustering Information*, we see that the table is clustered by **id** and **FlightNum**.\n",
    "    - At the bottom of the results, the **Table Properties** contain a variety of properties for liquid clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "211cc03e-249c-4165-8630-22e4e47b6588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE TABLE EXTENDED flights_cluster_id_flightnum;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80d85228-a6cc-4c05-873c-22c6b85c5a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### C3.2.1 Unique Carrier Column Query\n",
    "1. Run the following query to analyze the average arrival delay in flights for the *TW* airline. Take note of the time it took for the query to execute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "902dacaf-9d1b-4d22-8f32-47d6a3cc8f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Perform the SELECT operation on the 'flights_cluster_id' table with the AVG operation on the 'ArrDelay' column and the 'UniqueCarrier' set to 'TW'.\n",
    "SELECT AVG(try_cast(ArrDelay AS DOUBLE)) \n",
    "FROM flights_cluster_id_flightnum\n",
    "WHERE UniqueCarrier = 'TW';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b2368aa-3588-4cda-9f32-274a839f9dc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Note in particular the amount of cloud storage requests and their associated time. To view how the query performed complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **SQL/DataFrame Properties** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotonScan parquet dbacademy_flightdata.v01.flights_cluster_id_flightnum (1)** and select the plus icon.\n",
    "\n",
    "\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary slightly):\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count total (min, med, max)| 374 (3, 10, 14)| Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data.|\n",
    "| cloud storage response size total (min, med, max)|997.6 MiB (988.3 KiB, 27.0 MiB, 43.7 MiB) | Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.|\n",
    "| files pruned | 0 |A total of 0 files were skipped by Spark due to pruning based on the query's filters. This is due to the table being clustered by **id** and **FlightNum** and queried by the **UniqueCarrier** column.|\n",
    "| files read | 144 | All files were read during the execution of the Spark job.|\n",
    "\n",
    "\n",
    "#### Summary\n",
    "This table was clustered by **id** and **FlightNum**, and queried by the **UniqueCarrier** column. This query should run in about ~10 seconds. Similar to the other two queries performed on the **UniqueCarrier** column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3f6e95a-b67b-42da-87fb-57fc2172bd45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### C3.2.2 FlightNum Column Query\n",
    "1. Run the following query to analyze the average arrival delay by flight number (**FlightNum**) *1809*. Take note of the time it took for the query to execute. Remember, the **flights_cluster_id_flightnum** table is clustered by **id** and **FlightNum**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def2fca2-c64d-44ef-9b80-b1db860daa85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Perform the SELECT operation on the 'flights_cluster_id' table with the AVG operation on the 'ArrDelay' column and the 'FlightNum' set to '1890'.\n",
    "SELECT AVG(try_cast(ArrDelay AS DOUBLE)) \n",
    "FROM flights_cluster_id_flightnum\n",
    "WHERE FlightNum = 1890;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f45524a0-f63a-4ead-9e2e-0ba0c1641a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Note in particular the amount of cloud storage requests and their associated time. To view how the query performed complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **SQL/DataFrame Properties** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotonScan parquet dbacademy_flightdata.v01.flights_cluster_id_flightnum (1)** and select the plus icon.\n",
    "\n",
    "\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary slightly):\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count total (min, med, max)| 32 (4, 6, 6)|Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data.|\n",
    "| cloud storage response size total (min, med, max)|146.3 MiB (21.9 MiB, 24.9 MiB, 27.6 MiB) | Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.|\n",
    "| files pruned | 132 |A total of 132 files were skipped by Spark due to pruning based on the query's filters. This is due to the table being clustered by **id** and **FlightNum**, and queried by the **FlightNum** column.|\n",
    "| files read | 12| Only 12 files were read during the execution of the Spark job.|\n",
    "\n",
    "\n",
    "#### Summary\n",
    "This table was clustered by **FlightNum** and **id** and queried by the **FlightNum** column. This enables Spark to optimize how it reads the data. This query should execute in around ~2 seconds, similar to the **flights** table with a ZORDER on **FlightNum**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e3c18d8-463a-46a9-ac8a-727512ec8909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### C3.2.3 id Column Query\n",
    "1. Run the following query to analyze the records in the flight table with the **id** *1125281431554*. Take note of the time it took for the query to execute. Remember, the **flights_cluster_id_flightnum** table is clustered by **id** and **FlightNum**.\n",
    "\n",
    "    Notice how quickly this query executes compared to the **flights** table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f83ceb3a-f3b3-4839-8598-565198186720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM flights_cluster_id_flightnum\n",
    "WHERE id = 1125281431554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ca9be8c-b7af-4013-ad45-19ca58af9adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Note in particular the amount of cloud storage requests and their associated time. To view how the query performed complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **SQL/DataFrame Properties** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotonScan parquet dbacademy_flightdata.v01.flights_cluster_id_flightnum (1)** and select the plus icon.\n",
    "\n",
    "\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary slightly):\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count total (min, med, max)| 29 (2, 6, 6)|  Tracks the number of read and write requests made to cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. Monitoring this metric helps optimize performance, reduce costs, and identify potential inefficiencies in data access patterns.|\n",
    "| cloud storage response size total (min, med, max)|370.9 MiB (27.3 MiB, 68.5 MiB, 71.5 MiB) | Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.|\n",
    "| files pruned | 134 |A total of 134 files were skipped by Spark due to pruning based on the query's filters. This is due to the table being clustered by **FlightNum** and **id**, and queried by the  **id** column.|\n",
    "| files read | 10 | All files were read during the execution of the Spark job.|\n",
    "\n",
    "\n",
    "#### Liquid Clustering Summary\n",
    "This table was clustered by **FlightNum** and **id** and queried by the **id** column. This enables Spark to optimize how it reads the data. This query should execute in around 2 seconds, much faster than the **flights** table with a ZORDER on **FlightNum** using the same query (~28 seconds).\n",
    "\n",
    "<br></br>\n",
    "## NEWS! Automatic Liquid Clustering (Public Preview as of March 2025)!\n",
    "\n",
    "- [Announcing Automatic Liquid Clustering: Optimized data layout for up to 10x faster queries](https://www.databricks.com/blog/announcing-automatic-liquid-clustering) (Blog)\n",
    "\n",
    "- [Automatic liquid clustering](https://docs.databricks.com/aws/en/delta/clustering#automatic-liquid-clustering) documentation\n",
    "\n",
    "![LQ Auto](./Includes/images/Liquid-Clusters-OG.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "772332bf-9e4b-4942-a595-d1b304c4587c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D. Lab Queries Summary (estimated average execution times below, execution times will vary)\n",
    "Please review the queries for each table below and compare some of the query statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0cb3509-eb74-4519-b1fc-7a6c95a31ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### D1. Query by UniqueCarrier\n",
    "**NOTES:** No storage optimization was set on the **UniqueCarrier** column, so all files were read for each table during the query. Each query had similar cloud storage response sizes and execution times.\n",
    "\n",
    "|Table| Query | Total Files | Files Read | Files Pruned | Query Duration (will vary) | cloud storage request count total | cloud storage response size total |\n",
    "|----------|----------|----------|----------|----------|----------|----------|----------|\n",
    "|**ZORDER by FlightNum**| `WHERE UniqueCarrier = 'TW'`    | 31   |31   |0   | ~12 s | 149  | 1068 MiB |\n",
    "|**CLUSTER BY id**| `WHERE UniqueCarrier = 'TW'`   | 128  | 128   | 0  | ~10 s | 316 | 1047.8 MiB |\n",
    "|**CLUSTER BY (id, FlightNum)**| `WHERE UniqueCarrier = 'TW'`  | 144  | 144   | 0 | ~10 s | 374  | 997.6 MiB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f66aff00-4180-4e48-a293-ad5755508990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### D2. Query by FlightNum\n",
    "**NOTES:** Notice that when the tables included **FlightNum** in a storage optimization technique, many of the files were pruned, increasing efficiency and speeding up execution times.\n",
    "\n",
    "The table that was only clustered by **id** ran slower, and the cloud storage response size was much larger.\n",
    "\n",
    "\n",
    "|Table| Query | Total Files | Files Read | Files Pruned | Query Duration (will vary) | cloud storage request count total | cloud storage response size total |\n",
    "|----------|----------|----------|----------|----------|----------|----------|----------|\n",
    "|**ZORDER by FlightNum**| `WHERE FlightNum = 1890`    | 31   |1   |30   | ~2 s | 6 |53.MiB |\n",
    "|**CLUSTER BY id**| `WHERE FlightNum = 1890`   | 128  | 128   | 0  | ~10 s\t |275  | **1871.3 MiB** |\n",
    "|**CLUSTER BY (id, FlightNum)**| `WHERE FlightNum = 1890`  | 144  | 12   | 132 | ~2 s| 32 | 146.3 MiB |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce4c9fcb-4bbd-428f-b571-6a488e6e6c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### D3. Query by id\n",
    "**NOTES:** Notice that when the tables included **id** in a storage optimization technique (both liquid clustered tables), many of the files were pruned, increasing efficiency, even in the table that is clustered by both **id** and **FlightNum**. \n",
    "\n",
    "You can also see that the cloud storage response size of each of the clustered tables was much smaller than the Z-ordered table by **FlightNum**.\n",
    "\n",
    "Also, notice that the Z-ordered table took an extremely long time to complete, and the cloud storage response size was much larger when querying the **id** column in the **flights** table.\n",
    "\n",
    "\n",
    "|Table| Query | Total Files | Files Read | Files Pruned | Query Duration (will vary) | cloud storage request count total | cloud storage response size total |\n",
    "|----------|----------|----------|----------|----------|----------|----------|----------|\n",
    "|**ZORDER by FlightNum**| `WHERE id = 1125281431554`     | 31   |31   |31   | **~28 s** |**267**  | **4.7 GiB** |\n",
    "|**CLUSTER BY id**| `WHERE id = 1125281431554`    | 128  | 1   | 127  | ~2 s | 4 |54.5 MiB |\n",
    "|**CLUSTER BY (id, FlightNum)**| `WHERE id = 1125281431554`   | 144  | 10   | 134 | ~2 s | 29 | 370.9 MiB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d88cdd6-b1bf-4580-b7b5-b7c2fce234df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "PO 1.2L - Data Skipping and Liquid Clustering",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "SQL",
   "language": "sql",
   "name": "sql"
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
