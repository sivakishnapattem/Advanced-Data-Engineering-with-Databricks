{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b613835-574f-4b66-8f58-2af0bfcb465d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../Includes/_common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76177d57-ad1e-4e93-a653-4a4aa91888dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "DA = DBAcademyHelper()\n",
    "DA.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0751109-9b8d-4142-aad9-427567fedad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.paths.sales = f\"{DA.paths.datasets.ecommerce}/delta/sales_hist\"\n",
    "DA.paths.users = f\"{DA.paths.datasets.ecommerce}/delta/users_hist\"\n",
    "DA.paths.events = f\"{DA.paths.datasets.ecommerce}/delta/events_hist\"\n",
    "DA.paths.products = f\"{DA.paths.datasets.ecommerce}/delta/item_lookup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8357a315-c72f-4b96-8fb3-2267fbd3debc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def stop_streaming_query(self):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        str: A message indicating the result of the operation.\n",
    "    \"\"\"\n",
    "    # Iterate through all active streaming queries\n",
    "    for query in spark.streams.active:\n",
    "        query.stop()  # Stop the matching query\n",
    "        result = f\"Stopped streaming query: {query.name}\"\n",
    "        print(result)\n",
    "    \n",
    "    # If no matching query is found\n",
    "    if result == None:\n",
    "        return f\"No active streaming query found \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06d946aa-58c4-4afd-8204-dd0d53fdd116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def validate_dataframe(self, df, required_columns):\n",
    "    \"\"\"\n",
    "    Simple validation function for a DataFrame.\n",
    "    - Checks if the DataFrame is streaming.\n",
    "    - Validates the presence of required columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to validate.\n",
    "    - required_columns: List of columns that are required in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - None: Raises an exception if any test fails.\n",
    "    \"\"\"\n",
    "    # Check if the DataFrame is streaming\n",
    "    if not df.isStreaming:\n",
    "        print(\"Test Failed!\")\n",
    "        raise AssertionError(\"The DataFrame is not streaming.\")\n",
    "    \n",
    "    # Check if all required columns are present\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise AssertionError(f\"Missing columns in DataFrame: {missing_columns}\")\n",
    "    \n",
    "    print(\"All validations passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb2fb48a-7c26-4b19-80e7-27aaa6e0bc3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def validate_schema(self,schema,expected_fields):\n",
    "    \"\"\"\n",
    "    Simple schema validation function.\n",
    "    - Ensures schema has the correct fields and types.\n",
    "\n",
    "    Parameters:\n",
    "    - schema: StructType to validate.\n",
    "\n",
    "    Raises:\n",
    "    - AssertionError if validation fails.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.types import StructType, LongType, StringType, DoubleType\n",
    "\n",
    "    # Check if schema is a StructType\n",
    "    if not isinstance(schema, StructType):\n",
    "        print('Test Failed!')\n",
    "        raise AssertionError(f\"Schema is not of type StructType. Found {type(schema)}.\")\n",
    "\n",
    "\n",
    "    # Validate each field\n",
    "    for field in schema:\n",
    "        field_name = field.name\n",
    "        field_type = type(field.dataType).__name__\n",
    "\n",
    "        if field_name not in expected_fields:\n",
    "            raise AssertionError(f\"Unexpected field: {field_name}.\")\n",
    "        if expected_fields[field_name] != field_type:\n",
    "            raise AssertionError(f\"Field {field_name} has incorrect type. Expected {expected_fields[field_name]}, found {field_type}.\")\n",
    "\n",
    "    print(\"Schema validation passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1397512b-5759-4651-a690-d5dc74988976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def validate_query_state(self,query):\n",
    "    # Check if the query is not active\n",
    "    if query.isActive:\n",
    "        print(\"Test Failed: The query is active.\")\n",
    "    else:\n",
    "        print(\"Test Passed: The query is not active.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c6c383c-f243-4987-8dd5-1981b499f0fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def validate_query_status(self,query_status):\n",
    "    # Check if the query status contains the correct keys\n",
    "    expected_keys = [\"message\", \"isDataAvailable\", \"isTriggerActive\"]\n",
    "    actual_keys = list(query_status.keys())\n",
    "    if sorted(actual_keys) != sorted(expected_keys):\n",
    "        print(f\"Test Failed: Invalid query status. Found {actual_keys}. Expected {expected_keys}.\")\n",
    "    else:\n",
    "        print(\"Test Passed: Valid query status.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afbd1f92-024a-4a58-8aa8-ed8efadff17b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DA.paths.sales = \"/Volumes/dbacademy_ecommerce/v01/delta/sales_hist/\"\n",
    "# DA.paths.users = \"/Volumes/dbacademy_ecommerce/v01/delta/users_hist/\"\n",
    "# DA.paths.events = \"/Volumes/dbacademy_ecommerce/v01/delta/events_hist/\"\n",
    "# DA.paths.products = \"/Volumes/dbacademy_ecommerce/v01/delta/item_lookup/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ef67508-f7ba-4ae7-a740-8e20df2b0027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DA.paths.sales = f\"{DA.paths.datasets.ecommerce}/delta/sales_hist\"\n",
    "# DA.paths.users = f\"{DA.paths.datasets.ecommerce}/delta/users_hist\"\n",
    "# DA.paths.events = f\"{DA.paths.datasets.ecommerce}/delta/events_hist\"\n",
    "# DA.paths.products = f\"{DA.paths.datasets.ecommerce}/delta/item_lookup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c83904e6-75e7-40c3-abf1-14ea7a7cbe68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(DA.paths.sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db78701a-4da9-4ccf-8366-66397670193d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # The DataFactory is just a pattern to demonstrate a fake stream is more of a function\n",
    "# # streaming workloads than it is of a pipeline - this pipeline happens to stream data.\n",
    "# class DataFactory:\n",
    "#     def __init__(self):\n",
    "        \n",
    "#         # Bind the stream-source to DA because we will use it again later.\n",
    "#         DA.paths.stream_source = f\"{DA.paths.working_dir}/stream-source\"\n",
    "        \n",
    "#         self.source_dir = f\"{DA.paths.datasets.retail}/retail-pipeline\"\n",
    "#         self.target_dir = DA.paths.stream_source\n",
    "        \n",
    "#         # All three datasets *should* have the same count, but just in case,\n",
    "#         # We are going to take the smaller count of the three datasets\n",
    "#         orders_count = len(dbutils.fs.ls(f\"{self.source_dir}/orders/stream_json\"))\n",
    "#         status_count = len(dbutils.fs.ls(f\"{self.source_dir}/status/stream_json\"))\n",
    "#         customer_count = len(dbutils.fs.ls(f\"{self.source_dir}/customers/stream_json\"))\n",
    "#         self.max_batch = min(min(orders_count, status_count), customer_count)\n",
    "        \n",
    "#         self.current_batch = 0\n",
    "        \n",
    "#     def load(self, continuous=False, delay_seconds=5):\n",
    "#         import time\n",
    "#         self.start = int(time.time())\n",
    "        \n",
    "#         if self.current_batch >= self.max_batch:\n",
    "#             print(\"Data source exhausted\\n\")\n",
    "#             return False\n",
    "#         elif continuous:\n",
    "#             while self.load():\n",
    "#                 time.sleep(delay_seconds)\n",
    "#             return False\n",
    "#         else:\n",
    "#             print(f\"Loading batch {self.current_batch+1} of {self.max_batch}\", end=\"...\")\n",
    "#             self.copy_file(\"customers\")\n",
    "#             self.copy_file(\"orders\")\n",
    "#             self.copy_file(\"status\")\n",
    "#             self.current_batch += 1\n",
    "#             print(f\"{int(time.time())-self.start} seconds\")\n",
    "#             return True\n",
    "            \n",
    "#     def copy_file(self, dataset_name):\n",
    "#         source_file = f\"{self.source_dir}/{dataset_name}/stream_json/{self.current_batch:02}.json/\"\n",
    "#         target_file = f\"{self.target_dir}/{dataset_name}/{self.current_batch:02}.json\"\n",
    "#         dbutils.fs.cp(source_file, target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e1cac68-14ff-42b5-9998-86c8308b38fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def delete_all_files(self):\n",
    "#   '''\n",
    "#     Utility method to delete all files from the folders in the stream-source volume to start over. Will loop over and delete any json file it finds in the specified folders within stream-source:\n",
    "#     Delete all files within every sub folder in the user's dbacademy.ops.username volume.\n",
    "#   '''\n",
    "#   ## List of all folders in users stream source path volume\n",
    "#   folder_path_in_volume = dbutils.fs.ls(f'{self.paths.stream_source}')\n",
    "\n",
    "#   ## Get all sub folders\n",
    "#   list_of_sub_folders = []\n",
    "#   for sub_folder in folder_path_in_volume:\n",
    "#       sub_folder_path = sub_folder.path.split(':')[1]\n",
    "#       list_of_sub_folders.append(sub_folder_path)\n",
    "    \n",
    "\n",
    "#   for folder in list_of_sub_folders:\n",
    "#     files_in_folder = dbutils.fs.ls(folder)\n",
    "#     for file in files_in_folder:\n",
    "#       file_to_delete = file.path.split(':')[1]\n",
    "#       print(f'Deleting file: {file_to_delete}')\n",
    "#       dbutils.fs.rm(file_to_delete)\n",
    "\n",
    "#   print(f'All files deleted in every sub folder within: {self.paths.stream_source}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0f1da5c-ce62-4c99-9f5d-c055a248807b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def print_pipeline_config(self, language):\n",
    "#     \"Provided by DBAcademy, this function renders the configuration of the pipeline as HTML\"\n",
    "\n",
    "#     config = self.get_pipeline_config(language)\n",
    "\n",
    "#     ## Create a list of tuples that indicate what notebooks the user needs to reference\n",
    "#     list_of_notebook_tuples = []\n",
    "#     for i, path in enumerate(config.notebooks):\n",
    "#         notebook = (f'Notebook #{i+1} Path', path)\n",
    "#         list_of_notebook_tuples.append(notebook)\n",
    "\n",
    "#     ## Use the display_config_values function to display the following values as HTML output.\n",
    "#     ## Will list the Pipeline Name, Source, Catalog, Target Schema and notebook paths.\n",
    "#     self.display_config_values([\n",
    "#             ('Pipeline Name',config.pipeline_name),\n",
    "#         ] + list_of_notebook_tuples \n",
    "#           + [('Catalog',self.catalog_name),\n",
    "#              ('Target Schema',self.schema_name), \n",
    "#              ('Source',config.source)]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "756dddbf-e059-4ea1-ba28-96f958b4976f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def print_pipeline_job_info(self):\n",
    "#     \"\"\"\n",
    "#     Returns the name of the job for the user to use. Unique schema name +: Example Pipeline.\n",
    "#     \"\"\"\n",
    "#     unique_name = self.unique_name(sep=\"-\")\n",
    "#     pipeline_name = f\"{unique_name}\"\n",
    "    \n",
    "#     pipeline_name += \": Example Pipeline\"\n",
    "   \n",
    "#     pipeline_name = pipeline_name.replace('-','_')\n",
    "#     print(f\"{pipeline_name}\")\n",
    "#     return pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c48ead80-5552-4397-b374-b8473e1b88ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def validate_pipeline_config(self, pipeline_language, num_notebooks=3):\n",
    "#     \"Provided by DBAcademy, this function validates the configuration of the pipeline\"\n",
    "#     import json\n",
    "    \n",
    "#     config = self.get_pipeline_config(pipeline_language)\n",
    "\n",
    "#     try:\n",
    "#         pipeline = self.workspace.pipelines.get(\n",
    "#             self.workspace_find(\n",
    "#                 'pipelines',\n",
    "#                 config.pipeline_name,\n",
    "#                 api='list_pipelines'\n",
    "#             ).pipeline_id\n",
    "#         )\n",
    "#     except:\n",
    "#         assert False, f\"Could not find a pipeline named {config.pipeline_name}. Please name your pipeline using the information provided in the print_pipeline_config output.\"\n",
    "\n",
    "#     assert pipeline is not None, \"Could not find a pipeline named {config.pipeline_name}\"\n",
    "#     assert pipeline.spec.catalog == self.catalog_name, f\"Catalog not set to {self.catalog_name}\"\n",
    "#     assert pipeline.spec.target == self.schema_name, f\"Target schema not set to {self.schema_name}\"\n",
    "\n",
    "#     libraries = [l.notebook.path for l in pipeline.spec.libraries]\n",
    "    \n",
    "#     def test_notebooks():\n",
    "#         if libraries is None: return False\n",
    "#         if len(libraries) != num_notebooks: return False\n",
    "#         for library in libraries:\n",
    "#             if library not in config.notebooks: return False\n",
    "#         return True\n",
    "    \n",
    "#     assert test_notebooks(), \"Notebooks are not properly configured\"\n",
    "#     assert len(pipeline.spec.configuration) == 1, \"Expected exactly one configuration parameter.\"\n",
    "#     assert pipeline.spec.configuration.get(\"source\") == config.source, f\"Expected the configuration parameter {config.source}\"\n",
    "#     assert pipeline.spec.channel == \"CURRENT\", \"Excpected the channel to be set to Current.\"\n",
    "#     assert pipeline.spec.continuous == False, \"Expected the Pipeline mode to be Triggered.\"\n",
    "\n",
    "#     print('Pipeline validation complete. No errors found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06d47cc3-c7e8-4d74-8685-dcc96814e1a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def get_pipeline_config(self, language):\n",
    "#     \"\"\"\n",
    "#     Returns the configuration to be used by the student in configuring the pipeline.\n",
    "#     \"\"\"\n",
    "#     base_path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
    "#     base_path = \"/\".join(base_path.split(\"/\")[:-1])\n",
    "    \n",
    "#     pipeline_name = self.print_pipeline_job_info()\n",
    "\n",
    "#     if language is None: language = dbutils.widgets.getArgument(\"pipeline-language\", None)\n",
    "#     assert language in [\"SQL\", \"Python\"], f\"A valid language must be specified, found {language}\"\n",
    "    \n",
    "#     AB = \"A\" if language == \"SQL\" else \"B\"\n",
    "#     return PipelineConfig(pipeline_name, self.paths.stream_source, [\n",
    "#         f\"{base_path}/2{AB} - {language} Pipelines/1 - Orders Pipeline\",\n",
    "#         f\"{base_path}/2{AB} - {language} Pipelines/2 - Customers Pipeline\",\n",
    "#         f\"{base_path}/2{AB} - {language} Pipelines/3L - Status Pipeline Lab\"\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e8979f6-97af-45ff-917e-d617e43574d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def generate_pipeline_name(self):\n",
    "#     return DA.schema_name.replace('_','-') + \": Example Pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec57d4bd-2b82-4b58-b3d0-61cc48914d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def generate_pipeline(self,\n",
    "#                       pipeline_name, \n",
    "#                       notebooks_folder, \n",
    "#                       pipeline_notebooks,\n",
    "#                       use_schema,\n",
    "#                       use_configuration = None,\n",
    "#                       use_serverless = True,\n",
    "#                       use_continuous = False):\n",
    "#     \"\"\"\n",
    "#     Generates a Databricks pipeline based on the specified configuration parameters.\n",
    "\n",
    "#     This method creates a pipeline that can execute a series of notebooks in a serverless environment, \n",
    "#     and allows the option to use continuous runs if needed. It relies on Databricks SDK to interact with Databricks services.\n",
    "\n",
    "#     By default will use the dbacademy catalog, within the user's specific schema.\n",
    "\n",
    "#     Parameters:\n",
    "#     - pipeline_name (str): The name of the pipeline to be created.\n",
    "#     - notebooks_folder (str): The folder within Databricks where the notebooks are stored. This should be the folder name one level above where the Classroom-Setup-Common notebooks lives.\n",
    "#     - pipeline_notebooks (list of str): List of notebook paths that should be included in the pipeline. Use path from after the notebooks_folder.\n",
    "#     - use_configuration (dict or None): Optional configuration dictionary that can be used to customize the pipeline's settings. \n",
    "#         - Default is None.\n",
    "#     - use_serverless (bool): Flag indicating whether to use the serverless environment for the pipeline. \n",
    "#         - Defaults to True.\n",
    "#     - use_continuous (bool): Flag indicating whether to set up continuous execution for the pipeline. \n",
    "#         - Defaults to False.\n",
    "\n",
    "#     Returns:\n",
    "#     - pipeline (object): A Databricks pipeline object created using the specified parameters.\n",
    "#     - Stores the pipeline_id in the self.current_pipeline_id attribute.\n",
    "\n",
    "#     Raises:\n",
    "#     - Raises an error if the pipeline name already exists.\n",
    "\n",
    "#     Example usage:\n",
    "#             DA.generate_pipeline(\n",
    "#                 pipeline_name=f\"DEV\",            ## creates a pipeline catalogname_DEV\n",
    "#                 use_schema = 'default',          ## uses schema within user's catalog\n",
    "#                 notebooks_folder='Pipeline 01', \n",
    "#                 pipeline_notebooks=[            ## Uses Pipeline 01/bronze/dev/ingest_subset\n",
    "#                     'bronze/dev/ingest_subset',\n",
    "#                     'silver/quarantine'\n",
    "#                     ]\n",
    "#                 )\n",
    "    \n",
    "#     Notes:\n",
    "#     - The method imports the necessary Databricks SDK service for pipelines.\n",
    "#     - The 'use_catalog' and 'use_schema' attributes are assumed to be part of the class, and are used to define catalog and schema name using the customer DA object attributes.\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     from databricks.sdk.service import pipelines\n",
    "    \n",
    "#     ## Set pipeline name and target catalog\n",
    "#     pipeline_name = f\"{pipeline_name}\" \n",
    "#     use_catalog = f\"{self.catalog_name}\"\n",
    "    \n",
    "#     ## Check for duplicate name. Return error if name already found.\n",
    "#     for pipeline in self.workspace.pipelines.list_pipelines():\n",
    "#       if pipeline.name == pipeline_name:\n",
    "#         assert_false = False\n",
    "#         assert assert_false, f'You already have pipeline named {pipeline_name}. Please go to the Delta Live Tables page and manually delete the pipeline. Then rerun this program to create the pipeline.'\n",
    "\n",
    "#     ## Get path of includes folder\n",
    "#     current_folder_path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
    "\n",
    "#     ## Go back one folder to the main course folder then navigate to the folder specified by the notebooks_folder variable\n",
    "#     main_course_folder_path = \"/Workspace\" + \"/\".join(current_folder_path.split(\"/\")[:-1]) + '/' + notebooks_folder\n",
    "\n",
    "#     ## Create paths for each notebook specified in method argument notebooks(list of notebooks to use)\n",
    "#     notebooks_paths = []\n",
    "#     for i, notebook in enumerate(pipeline_notebooks):\n",
    "#         current_notebook_path = (f'Notebook #{i + 1}', main_course_folder_path + '/' + notebook)\n",
    "        \n",
    "#         # Attempt to list the contents of the path. If the path does not exist return an error.\n",
    "#         if os.path.exists(current_notebook_path[1]):\n",
    "#             pass\n",
    "#         else:\n",
    "#             assert_false = False\n",
    "#             assert assert_false, f'The notebook path you specified does not exists {current_notebook_path[1]}. Please specify a correct path in the generate_pipeline() method using the notebooks_folder and pipeline_notebooks arguments. Read the method documentation for more information.'\n",
    "        \n",
    "#         notebooks_paths.append(current_notebook_path)\n",
    "\n",
    "\n",
    "#     ## Create pipeline\n",
    "#     pipeline_info = self.workspace.pipelines.create(\n",
    "#         allow_duplicate_names=True,\n",
    "#         name=pipeline_name,\n",
    "#         catalog=use_catalog,\n",
    "#         target=use_schema,\n",
    "#         serverless=use_serverless,\n",
    "#         continuous=use_continuous,\n",
    "#         configuration=use_configuration,\n",
    "#         libraries=[pipelines.PipelineLibrary(notebook=pipelines.NotebookLibrary(path=notebook)) for i, notebook in notebooks_paths]\n",
    "#     )\n",
    "\n",
    "#     ## Store pipeline ID\n",
    "#     self.current_pipeline_id = pipeline_info.pipeline_id \n",
    "\n",
    "#     ## Success message\n",
    "#     print(f\"Created the DLT pipeline {pipeline_name} using the settings from below:\\n\")\n",
    "\n",
    "#     ## Use the display_config_values function to display the following values as HTML output.\n",
    "#     ## Will list the Job Name and notebook paths.\n",
    "#     self.display_config_values([\n",
    "#             ('DLT Pipeline Name', pipeline_name),\n",
    "#             ('Using Catalog', self.catalog_name),\n",
    "#             ('Using Schema', use_schema),\n",
    "#             ('Compute', 'Serverless' if use_serverless else 'Error in setting Compute')\n",
    "#         ] + notebooks_paths)\n",
    "    \n",
    "\n",
    "# @DBAcademyHelper.add_method\n",
    "# def start_pipeline(self):\n",
    "#     '''\n",
    "#     Starts the pipeline using the attribute set from the generate_pipeline() method.\n",
    "#     '''\n",
    "#     print('Started the pipeline run. Navigate to Delta Live Tables to view the pipeline.')\n",
    "#     self.workspace.pipelines.start_update(self.current_pipeline_id)\n",
    "\n",
    "\n",
    "# # # Example METHOD\n",
    "# # DA.generate_pipeline(\n",
    "# #     pipeline_name=f\"DEV1\", \n",
    "# #     use_schema = 'default',\n",
    "# #     notebooks_folder='Pipeline 01', \n",
    "# #     pipeline_notebooks=[\n",
    "# #         'bronze/dev/ingest_subset',\n",
    "# #         'silver/quarantine'\n",
    "# #         ]\n",
    "# #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a596620a-a579-4c1f-9bf2-6496a793bf23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# class PipelineConfig():\n",
    "#     def __init__(self, pipeline_name, source, notebooks):\n",
    "#         self.pipeline_name = pipeline_name # The name of the pipeline\n",
    "#         self.source = source               # Custom Property\n",
    "#         self.notebooks = notebooks         # This list of notebooks for this pipeline\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         content = f\"Name:      {self.pipeline_name}\\nSource:    {self.source}\\n\"\"\"\n",
    "#         content += f\"Notebooks: {self.notebooks.pop(0)}\"\n",
    "#         for notebook in self.notebooks: content += f\"\\n           {notebook}\"\n",
    "#         return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da469464-a5a4-4b4a-80f9-2321401e16b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def create_pipeline(self, language):\n",
    "#     \"Provided by DBAcademy, this function creates the prescribed pipeline\"\n",
    "    \n",
    "#     config = self.get_pipeline_config(language)\n",
    "\n",
    "#     # Delete the existing pipeline if it exists\n",
    "#     try:\n",
    "#         self.workspace.pipelines.delete(\n",
    "#             self.workspace_find(\n",
    "#                 'pipelines',\n",
    "#                 config.pipeline_name,\n",
    "#                 api='list_pipelines'\n",
    "#             ).pipeline_id\n",
    "#         )\n",
    "#     except NotFound:\n",
    "#         pass\n",
    "\n",
    "#     policy = self.get_dlt_policy()\n",
    "#     if policy is None: cluster = [{\"num_workers\": 1}]\n",
    "#     else:              cluster = [{\"num_workers\": 1, \"policy_id\": self.get_dlt_policy().get(\"policy_id\")}]\n",
    "    \n",
    "#     # Create the new pipeline\n",
    "#     self.pipeline_id = self.workspace.pipelines.create(\n",
    "#         name=config.pipeline_name, \n",
    "#         development=True,\n",
    "#         catalog=self.catalog_name,\n",
    "#         target=self.schema_name,\n",
    "#         notebooks=config.notebooks,\n",
    "#         configuration = {\n",
    "#             \"source\": config.source\n",
    "#         },\n",
    "#         clusters=cluster\n",
    "#     ).pipeline_id\n",
    "\n",
    "#     print(f\"Created the pipeline \\\"{config.pipeline_name}\\\" ({self.pipeline_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9342cedb-0b32-4d72-9afa-7af4f6f9940a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def start_pipeline(self):\n",
    "#     \"Starts the pipeline and then blocks until it has completed, failed or was canceled\"\n",
    "\n",
    "#     import time\n",
    "\n",
    "#     # Start the pipeline\n",
    "#     update_id = self.workspace.pipelines.start_update(self.pipeline_id).update_id\n",
    "\n",
    "#     # Get the status and block until it is done\n",
    "#     state = self.workspace.pipelines.get_update(self.pipeline_id, update_id).update.state.value\n",
    "\n",
    "#     duration = 15\n",
    "\n",
    "#     while state not in [\"COMPLETED\", \"FAILED\", \"CANCELED\"]:\n",
    "#         print(f\"Current state is {state}, sleeping {duration} seconds.\")    \n",
    "#         time.sleep(duration)\n",
    "#         state = self.workspace.pipelines.get_update(self.pipeline_id, update_id).update.state.value\n",
    "    \n",
    "#     print(f\"The final state is {state}.\")    \n",
    "#     assert state == \"COMPLETED\", f\"Expected the state to be COMPLETED, found {state}\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-Common",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
