{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a16b163-264a-4dc2-b88e-836dafee019b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../Includes/_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f36eaa6e-8986-4d04-af8b-023aed2fc1dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "DA = DBAcademyHelper()\n",
    "DA.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90291d60-456d-4782-a22e-fa18b39d8b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class StreamFactory:\n",
    "    \"\"\"\n",
    "    Incrementally loads data from a source dataset to a target directory.\n",
    "\n",
    "    Attributes:\n",
    "        source_dir: source path for datasets\n",
    "        target_dir: landing path for streams\n",
    "        max_batch: total number of batches before exhausting stream\n",
    "        batch: counter used to track current batch number\n",
    "\n",
    "    Methods:\n",
    "        load(continuous=False)\n",
    "        load_batch(target, batch, end): dataset-specific function provided at instantiation\n",
    "    \"\"\"\n",
    "    def __init__(self, source_dir, target_dir, load_batch, max_batch):\n",
    "        self.source_dir = source_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.load_batch = load_batch\n",
    "        self.max_batch = max_batch\n",
    "        self.batch = 1     \n",
    "        \n",
    "    def load(self, continuous=False):\n",
    "        \n",
    "        if self.batch > self.max_batch:\n",
    "            print(\"Data source exhausted\", end=\"...\")\n",
    "            total = 0                \n",
    "        elif continuous == True:\n",
    "            print(f\"Loading all batches to the stream\", end=\"...\")\n",
    "            total = self.load_batch(self.source_dir, self.target_dir, self.batch, self.max_batch)\n",
    "            self.batch = self.max_batch + 1\n",
    "        else:\n",
    "            print(f\"Loading batch #{self.batch} to the stream\", end=\"...\")\n",
    "            total = self.load_batch(self.source_dir, self.target_dir, self.batch, self.batch)\n",
    "            self.batch = self.batch + 1\n",
    "            \n",
    "        print(f\"Loaded {total:,} records\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56a86b97-e004-4d47-99a1-b42104b8637d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def load_user_reg_batch(self,datasets_dir, target_dir, batch_start, batch_end):\n",
    "\n",
    "    source_dataset = f\"{datasets_dir}/user-reg\"\n",
    "    target_path = f\"{target_dir}/user_reg\"\n",
    "\n",
    "    df = (spark.read\n",
    "          .format(\"json\")\n",
    "          .schema(\"device_id long, mac_address string, registration_timestamp double, user_id long\")\n",
    "          .load(source_dataset)\n",
    "          .withColumn(\"date\", F.col(\"registration_timestamp\").cast(\"timestamp\").cast(\"date\"))\n",
    "          .withColumn(\"batch\", F.when(F.col(\"date\") < \"2019-12-01\", F.lit(1)).otherwise(F.dayofmonth(F.col(\"date\"))+1))\n",
    "          .filter(f\"batch >= {batch_start}\")\n",
    "          .filter(f\"batch <= {batch_end}\")          \n",
    "          .drop(\"date\", \"batch\")\n",
    "          .cache())\n",
    "\n",
    "    df.write.mode(\"append\").format(\"json\").save(target_path)\n",
    "    return df.count()        \n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def load_cdc_batch(self,datasets_dir, target_dir, batch_start, batch_end):\n",
    "    \n",
    "    source_dataset = f\"{datasets_dir}/pii/raw\"\n",
    "    target_path = f\"{target_dir}/cdc\"\n",
    "\n",
    "    df = (spark.read\n",
    "      .load(source_dataset)\n",
    "      .filter(f\"batch >= {batch_start}\")\n",
    "      .filter(f\"batch <= {batch_end}\")\n",
    "    )   \n",
    "    df.write.mode(\"append\").format(\"json\").save(target_path)\n",
    "    return df.count()\n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def load_daily_batch(datasets_dir, target_dir, batch_start, batch_end):\n",
    "    \n",
    "    source_path = f\"{datasets_dir}/bronze\"\n",
    "    target_path = f\"{target_dir}/daily\"\n",
    "\n",
    "\n",
    "    df = (spark.read\n",
    "      .load(source_path)\n",
    "      .withColumn(\"day\", \n",
    "        F.when(F.col(\"date\") <= '2019-12-01', 1)\n",
    "        .otherwise(F.dayofmonth(\"date\")))\n",
    "      .filter(F.col(\"day\") >= batch_start)\n",
    "      .filter(F.col(\"day\") <= batch_end)\n",
    "      .drop(\"date\", \"week_part\", \"day\")  \n",
    "    )\n",
    "    df.write.mode(\"append\").format(\"json\").save(target_path)\n",
    "    return df.count()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a7d9034-404a-40d4-af15-2b25559fb21a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def clone_table(self,table_name, source_path =None, source_name=None):\n",
    "    \"\"\"\n",
    "    Clones a Delta table using a shallow clone.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Name of the target table to create.\n",
    "        source_path (str): Path to the source Delta table.\n",
    "        source_name (str, optional): Specific name of the source table within the source path. Defaults to the target table name.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from time import perf_counter\n",
    "\n",
    "    start_time = perf_counter()\n",
    "\n",
    "    # Use the table name as the source name if not provided\n",
    "    if source_name is None:\n",
    "        source_name = table_name\n",
    "\n",
    "    if source_path is None:\n",
    "        source_path = DA.paths.datasets.gym\n",
    "\n",
    "    table_exists = spark.catalog.tableExists(f\"dbacademy.{DA.schema_name}.date_lookup_clone\")\n",
    "    \n",
    "    if table_exists == False:\n",
    "        print(f\"Cloning table '{table_name}' from '/Volumes/dbacademy_gym/v01/{source_name}'...\")\n",
    "\n",
    "        # Execute the shallow clone SQL command\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {table_name}\n",
    "            CLONE delta.`{source_path}/{source_name}`\n",
    "        \"\"\")\n",
    "\n",
    "        elapsed_time = perf_counter() - start_time\n",
    "        print(f\"Table '{table_name}' cloned successfully in {elapsed_time:.2f} seconds.\")\n",
    "    else:\n",
    "        print(f\"Table 'dbacademy.{DA.schema_name}.{table_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d50024b-3d42-4570-958f-051b08a44633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def generate_pipeline_name(self):\n",
    "    return DA.schema_name.replace('-','_') + \"_pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8466291-bcec-4d94-a3da-12aafb8fa33b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "class DeclarativePipelineCreator:\n",
    "    \"\"\"\n",
    "    A class to create a Lakeflow Declarative DLT pipeline using the Databricks REST API.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    pipeline_name : str\n",
    "        Name of the pipeline to be created.\n",
    "    root_path_folder_name : str\n",
    "        The folder containing the pipeline code relative to current working directory.\n",
    "    source_folder_names : list\n",
    "        List of subfolders inside the root path containing source notebooks or scripts.\n",
    "    catalog_name : str\n",
    "        The catalog where the pipeline tables will be stored.\n",
    "    schema_name : str\n",
    "        The schema (aka database) under the catalog.\n",
    "    serverless : bool\n",
    "        Whether to use serverless compute.\n",
    "    configuration : dict\n",
    "        Optional key-value configurations passed to the pipeline.\n",
    "    continuous : bool\n",
    "        If True, enables continuous mode (streaming).\n",
    "    photon : bool\n",
    "        Whether to use Photon execution engine.\n",
    "    channel : str\n",
    "        The DLT release channel to use (e.g., \"PREVIEW\", \"CURRENT\").\n",
    "    development : bool\n",
    "        Whether to run the pipeline in development mode.\n",
    "    pipeline_type : str\n",
    "        Type of pipeline (e.g., 'WORKSPACE').\n",
    "    delete_pipeine_if_exists : bool\n",
    "        Delete the pipeline if it exists if True. Otherwise return an error (False).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pipeline_name: str,\n",
    "                 root_path_folder_name: str,\n",
    "                 catalog_name: str,\n",
    "                 schema_name: str,\n",
    "                 source_folder_names: list = None,\n",
    "                 serverless: bool = True,\n",
    "                 configuration: dict = None,\n",
    "                 continuous: bool = False,\n",
    "                 photon: bool = True,\n",
    "                 channel: str = 'CURRENT',\n",
    "                 development: bool = True,\n",
    "                 pipeline_type: str = 'WORKSPACE',\n",
    "                 delete_pipeine_if_exists = False):\n",
    "        \n",
    "        # Assign all input arguments to instance attributes\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.root_path_folder_name = root_path_folder_name\n",
    "        self.source_folder_names = source_folder_names or []\n",
    "        self.catalog_name = catalog_name\n",
    "        self.schema_name = schema_name\n",
    "        self.serverless = serverless\n",
    "        self.configuration = configuration or {}\n",
    "        self.continuous = continuous\n",
    "        self.photon = photon\n",
    "        self.channel = channel\n",
    "        self.development = development\n",
    "        self.pipeline_type = pipeline_type\n",
    "        self.delete_pipeine_if_exists = delete_pipeine_if_exists\n",
    "\n",
    "        # Instantiate the WorkspaceClient to communicate with Databricks REST API\n",
    "        self.workspace = WorkspaceClient()\n",
    "        self.pipeline_body = {}\n",
    "\n",
    "    def _check_pipeline_exists(self):\n",
    "        \"\"\"\n",
    "        Checks if a pipeline with the same name already exists. If specified, will delete the existing pipeline.\n",
    "        Raises:\n",
    "            ValueError if the pipeline already exists if delete_pipeine_if_exists is set to False. Otherwise deletes pipeline.\n",
    "        \"\"\"\n",
    "        ## Get a list of pipeline names\n",
    "        list_of_pipelines = self.workspace.pipelines.list_pipelines()\n",
    "\n",
    "        ## Check to see if the pipeline name already exists. Depending on the value of delete_pipeine_if_exists, either raise an error or delete the pipeline.\n",
    "        for pipeline in list_of_pipelines:\n",
    "            if pipeline.name == self.pipeline_name:\n",
    "                if self.delete_pipeine_if_exists == True:\n",
    "                    print(f'Pipeline with that name already exists. Deleting the pipeline {self.pipeline_name} and then recreating it.\\n')\n",
    "                    self.workspace.pipelines.delete(pipeline.pipeline_id)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Lakeflow Declarative Pipeline name '{self.pipeline_name}' already exists. \"\n",
    "                        \"Please delete the pipeline using the UI and rerun to recreate.\"\n",
    "                    )\n",
    "\n",
    "    def _build_pipeline_body(self):\n",
    "        \"\"\"\n",
    "        Constructs the body of the pipeline creation request based on class attributes.\n",
    "        \"\"\"\n",
    "        # Get current working directory\n",
    "        cwd = os.getcwd()\n",
    "\n",
    "        # Create full path to root folder\n",
    "        root_path = os.path.join('/', cwd, self.root_path_folder_name)\n",
    "\n",
    "        # Convert source folder names into glob pattern paths for the DLT pipeline\n",
    "        # source_paths = [os.path.join(root_path, folder, '**') for folder in self.source_folder_names]\n",
    "        source_paths = [os.path.join(root_path, folder) for folder in self.source_folder_names]\n",
    "        libraries = [{'glob': {'include': path}} for path in source_paths]\n",
    "\n",
    "        # Build dictionary to be sent in the API request\n",
    "        self.pipeline_body = {\n",
    "            'name': self.pipeline_name,\n",
    "            'pipeline_type': self.pipeline_type,\n",
    "            'root_path': root_path,\n",
    "            'libraries': libraries,\n",
    "            'catalog': self.catalog_name,\n",
    "            'schema': self.schema_name,\n",
    "            'serverless': self.serverless,\n",
    "            'configuration': self.configuration,\n",
    "            'continuous': self.continuous,\n",
    "            'photon': self.photon,\n",
    "            'channel': self.channel,\n",
    "            'development': self.development\n",
    "        }\n",
    "\n",
    "    def get_pipeline_id(self):\n",
    "        \"\"\"\n",
    "        Returns the ID of the created pipeline.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'response'):\n",
    "            raise RuntimeError(\"Pipeline has not been created yet. Call create_pipeline() first.\")\n",
    "\n",
    "        return self.response.get(\"pipeline_id\")\n",
    "\n",
    "\n",
    "    def create_pipeline(self):\n",
    "        \"\"\"\n",
    "        Creates the pipeline on Databricks using the defined attributes.\n",
    "        \n",
    "        Returns:\n",
    "            dict: The response from the Databricks API after creating the pipeline.\n",
    "        \"\"\"\n",
    "        # Check for name conflicts\n",
    "        self._check_pipeline_exists()\n",
    "\n",
    "        # Build the body of the API request and creates self.pipeline_body variable\n",
    "        self._build_pipeline_body()\n",
    "\n",
    "        # Display information to user\n",
    "        print(f\"Creating the Lakeflow Declarative Pipeline '{self.pipeline_name}'.\\n\")\n",
    "        print(f\"Using root folder path: {self.pipeline_body['root_path']}\\n\")\n",
    "        print(f\"Using source folder path(s): {self.pipeline_body['libraries']}\\n\")\n",
    "\n",
    "        # Make the API call\n",
    "        self.response = self.workspace.api_client.do('POST', '/api/2.0/pipelines', body=self.pipeline_body)\n",
    "\n",
    "        # Notify of completion\n",
    "        print(f\"Lakeflow Declarative Pipeline Creation '{self.pipeline_name}' Complete!\")\n",
    "\n",
    "        return self.response\n",
    "\n",
    "\n",
    "    def start_pipeline(self):\n",
    "        '''\n",
    "        Starts the pipeline using the attribute set from the generate_pipeline() method.\n",
    "        '''\n",
    "        print('Started the pipeline run. Navigate to Jobs and Pipelines to view the pipeline.')\n",
    "        self.workspace.pipelines.start_update(self.get_pipeline_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "973f550c-b442-4af7-b629-701a0b70647d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delete_source_files(source_files: str):\n",
    "    \"\"\"\n",
    "    Deletes all files in the specified source volume.\n",
    "\n",
    "    This function iterates through all the files in the given volume,\n",
    "    deletes them, and prints the name of each file being deleted.\n",
    "\n",
    "    Parameters:\n",
    "    - source_files : str\n",
    "        The path to the volume containing the files to delete. \n",
    "        Use the {DA.paths.working_dir} to dynamically navigate to the user's volume location in dbacademy/ops/vocareumlab@name:\n",
    "            Example: DA.paths.working_dir = /Volumes/dbacademy/ops/vocareumlab@name\n",
    "\n",
    "    Returns:\n",
    "    - None. This function does not return any value. It performs file deletion and prints all files that it deletes. If no files are found it prints in the output.\n",
    "\n",
    "    Example:\n",
    "    - delete_source_files(f'{DA.paths.working_dir}/pii/stream_source/user_reg')\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "\n",
    "    print(f\"\\nSearching for files in the volume: '{source_files}'.\")\n",
    "    if os.path.exists(source_files):\n",
    "        list_of_files = sorted(os.listdir(source_files))\n",
    "    else:\n",
    "        list_of_files = None\n",
    "\n",
    "    if not list_of_files:  # Checks if the list is empty.\n",
    "        print(f\"No files found in the volume: '{source_files}'.\\n\")\n",
    "    else:\n",
    "        print(f\"Deleting all files in the volume: '{source_files}'.\\n\")\n",
    "        for file in list_of_files:\n",
    "            file_to_delete = source_files + file\n",
    "            dbutils.fs.rm(file_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "107c277f-04cd-4643-ac13-58b484b9a743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def generate_pipeline(self,\n",
    "#                       pipeline_name, \n",
    "#                       notebooks_folder, \n",
    "#                       pipeline_notebooks,\n",
    "#                       use_schema,\n",
    "#                       use_configuration = None,\n",
    "#                       use_serverless = True,\n",
    "#                       use_continuous = False):\n",
    "#     \"\"\"\n",
    "#     Generates a Databricks pipeline based on the specified configuration parameters.\n",
    "\n",
    "#     This method creates a pipeline that can execute a series of notebooks in a serverless environment, \n",
    "#     and allows the option to use continuous runs if needed. It relies on Databricks SDK to interact with Databricks services.\n",
    "\n",
    "#     By default will use the dbacademy catalog, within the user's specific schema.\n",
    "\n",
    "#     Parameters:\n",
    "#     - pipeline_name (str): The name of the pipeline to be created.\n",
    "#     - notebooks_folder (str): The folder within Databricks where the notebooks are stored. This should be the folder name one level above where the Classroom-Setup-Common notebooks lives.\n",
    "#     - pipeline_notebooks (list of str): List of notebook paths that should be included in the pipeline. Use path from after the notebooks_folder.\n",
    "#     - use_configuration (dict or None): Optional configuration dictionary that can be used to customize the pipeline's settings. \n",
    "#         - Default is None.\n",
    "#     - use_serverless (bool): Flag indicating whether to use the serverless environment for the pipeline. \n",
    "#         - Defaults to True.\n",
    "#     - use_continuous (bool): Flag indicating whether to set up continuous execution for the pipeline. \n",
    "#         - Defaults to False.\n",
    "\n",
    "#     Returns:\n",
    "#     - pipeline (object): A Databricks pipeline object created using the specified parameters.\n",
    "#     - Stores the pipeline_id in the self.current_pipeline_id attribute.\n",
    "\n",
    "#     Raises:\n",
    "#     - Raises an error if the pipeline name already exists.\n",
    "\n",
    "#     Example usage:\n",
    "#             DA.generate_pipeline(\n",
    "#                 pipeline_name=f\"DEV\",            ## creates a pipeline catalogname_DEV\n",
    "#                 use_schema = 'default',          ## uses schema within user's catalog\n",
    "#                 notebooks_folder='Pipeline 01', \n",
    "#                 pipeline_notebooks=[            ## Uses Pipeline 01/bronze/dev/ingest_subset\n",
    "#                     'bronze/dev/ingest_subset',\n",
    "#                     'silver/quarantine'\n",
    "#                     ]\n",
    "#                 )\n",
    "    \n",
    "#     Notes:\n",
    "#     - The method imports the necessary Databricks SDK service for pipelines.\n",
    "#     - The 'use_catalog' and 'use_schema' attributes are assumed to be part of the class, and are used to define catalog and schema name using the customer DA object attributes.\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     from databricks.sdk.service import pipelines\n",
    "    \n",
    "#     ## Set pipeline name and target catalog\n",
    "#     pipeline_name = f\"{pipeline_name}\" \n",
    "#     use_catalog = f\"{self.catalog_name}\"\n",
    "    \n",
    "#     ## Check for duplicate name. Return error if name already found.\n",
    "#     for pipeline in self.workspace.pipelines.list_pipelines():\n",
    "#       if pipeline.name == pipeline_name:\n",
    "#         assert_false = False\n",
    "#         assert assert_false, f'You already have pipeline named {pipeline_name}. Please go to the Pipelines under Data Engineering section, and manually delete the pipeline. Then rerun this program to create the pipeline.'\n",
    "\n",
    "#     ## Get path of includes folder\n",
    "#     current_folder_path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
    "\n",
    "#     ## Go back one folder to the main course folder then navigate to the folder specified by the notebooks_folder variable\n",
    "#     main_course_folder_path = \"/Workspace\" + \"/\".join(current_folder_path.split(\"/\")[:-1]) + '/' + notebooks_folder\n",
    "\n",
    "#     ## Create paths for each notebook specified in method argument notebooks(list of notebooks to use)\n",
    "#     notebooks_paths = []\n",
    "#     for i, notebook in enumerate(pipeline_notebooks):\n",
    "#         current_notebook_path = (f'Notebook #{i + 1}', main_course_folder_path + '/' + notebook)\n",
    "        \n",
    "#         # Attempt to list the contents of the path. If the path does not exist return an error.\n",
    "#         if os.path.exists(current_notebook_path[1]):\n",
    "#             pass\n",
    "#         else:\n",
    "#             assert_false = False\n",
    "#             assert assert_false, f'The notebook path you specified does not exists {current_notebook_path[1]}. Please specify a correct path in the generate_pipeline() method using the notebooks_folder and pipeline_notebooks arguments. Read the method documentation for more information.'\n",
    "        \n",
    "#         notebooks_paths.append(current_notebook_path)\n",
    "\n",
    "\n",
    "#     ## Create pipeline\n",
    "#     pipeline_info = self.workspace.pipelines.create(\n",
    "#         allow_duplicate_names=True,\n",
    "#         name=pipeline_name,\n",
    "#         catalog=use_catalog,\n",
    "#         target=use_schema,\n",
    "#         serverless=use_serverless,\n",
    "#         continuous=use_continuous,\n",
    "#         configuration=use_configuration,\n",
    "#         libraries=[pipelines.PipelineLibrary(notebook=pipelines.NotebookLibrary(path=notebook)) for i, notebook in notebooks_paths]\n",
    "#     )\n",
    "\n",
    "#     ## Store pipeline ID\n",
    "#     self.current_pipeline_id = pipeline_info.pipeline_id \n",
    "\n",
    "#     ## Success message\n",
    "#     print(f\"Created the DLT pipeline {pipeline_name} using the settings from below:\\n\")\n",
    "\n",
    "#     ## Use the display_config_values function to display the following values as HTML output.\n",
    "#     ## Will list the Job Name and notebook paths.\n",
    "#     self.display_config_values([\n",
    "#             ('DLT Pipeline Name', pipeline_name),\n",
    "#             ('Using Catalog', self.catalog_name),\n",
    "#             ('Using Schema', use_schema),\n",
    "#             ('Compute', 'Serverless' if use_serverless else 'Error in setting Compute')\n",
    "#         ] + notebooks_paths)\n",
    "    \n",
    "\n",
    "# @DBAcademyHelper.add_method\n",
    "# def start_pipeline(self):\n",
    "#     '''\n",
    "#     Starts the pipeline using the attribute set from the generate_pipeline() method.\n",
    "#     '''\n",
    "#     print('Started the pipeline run. Navigate to Jobs and Pipelines to view the pipeline.')\n",
    "#     self.workspace.pipelines.start_update(self.current_pipeline_id)\n",
    "\n",
    "\n",
    "# # # Example METHOD\n",
    "# # DA.generate_pipeline(\n",
    "# #     pipeline_name=f\"DEV1\", \n",
    "# #     use_schema = 'default',\n",
    "# #     notebooks_folder='Pipeline 01', \n",
    "# #     pipeline_notebooks=[\n",
    "# #         'bronze/dev/ingest_subset',\n",
    "# #         'silver/quarantine'\n",
    "# #         ]\n",
    "# #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ac1c548-fbaf-4896-9efc-661a79b88d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def delete_all_pipelines(self):\n",
    "    \"\"\"\n",
    "    Deletes all Delta Live Tables (DLT) pipelines in the workspace.\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    from databricks.sdk.service import pipelines\n",
    "\n",
    "    # List all pipelines\n",
    "    all_pipelines = self.workspace.pipelines.list_pipelines()\n",
    "\n",
    "    # Delete each pipeline\n",
    "    for pipeline in all_pipelines:\n",
    "        self.workspace.pipelines.delete(pipeline_id=pipeline.pipeline_id)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-Common",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
