{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a3a0939-1529-4d3c-b118-81d4ffafe32e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49bcd947-2271-4d9a-88db-b219d975dd2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Auto Load Data to Bronze\n",
    "\n",
    "The chief architect has decided that rather than connecting directly to Kafka, a source system will send raw records as JSON files to cloud object storage. In this notebook, you'll build a multiplex table that will ingest these records with Auto Loader and store the entire history of this incremental feed. The initial table will store data from all of our topics and have the following schema. \n",
    "\n",
    "| Field | Type |\n",
    "| --- | --- |\n",
    "| key | BINARY |\n",
    "| value | BINARY |\n",
    "| topic | STRING |\n",
    "| partition | LONG |\n",
    "| offset | LONG\n",
    "| timestamp | LONG |\n",
    "| date | DATE |\n",
    "| week_part | STRING |\n",
    "\n",
    "This single table will drive the majority of the data through the target architecture, feeding three interdependent data pipelines.\n",
    "\n",
    "<!-- <img src=\"https://files.training.databricks.com/images/ade/ADE_arch_bronze.png\" width=\"60%\" /> -->\n",
    "\n",
    "**NOTE**: Details on additional configurations for connecting to Kafka are available <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/kafka.html\" target=\"_blank\">here</a>.\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "- Describe a multiplex design\n",
    "- Apply Auto Loader to incrementally process records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee817050-25eb-4cbc-b3b8-4ea5b34db47f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "source = spark.conf.get(\"source\")\n",
    "lookup_db = spark.conf.get(\"lookup_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ae8b49a-6a75-4b29-800e-20923c1e0a54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.table(\n",
    "    table_properties={\n",
    "        \"pipelines.reset.allowed\": \"false\"\n",
    "    }\n",
    ")\n",
    "def date_lookup():\n",
    "    return spark.read.table(f\"{lookup_db}.date_lookup_clone\").select(\"date\", \"week_part\")\n",
    "\n",
    "\n",
    "@dp.table(\n",
    "    partition_cols=[\"topic\", \"week_part\"],\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.reset.allowed\": \"false\"\n",
    "    }\n",
    ")\n",
    "def bronze():\n",
    "    return (\n",
    "      spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .schema(\"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .load(f\"{source}/daily\")\n",
    "        .join(\n",
    "          F.broadcast(dp.read(\"date_lookup\")), \n",
    "          F.to_date((F.col(\"timestamp\")/1000).cast(\"timestamp\")) == F.col(\"date\"), \"left\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5313741-ee12-4991-b649-0b0985205441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.table\n",
    "def distinct_topics():\n",
    "    return dp.read(\"bronze\").select(\"topic\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45fcaab7-40d2-45ac-b891-73846dd001d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "SDLT 2.1.1 - Auto Load to Bronze",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
