{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "377fb18c-c994-4c67-8499-73c06afeddd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87837bca-fcb4-4ac6-ac55-512ac98c417d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "# Streaming ETL Lab\n",
    "## Process Workouts\n",
    "\n",
    "In this lab, you will configure a query to consume and parse raw data from a single topic as it lands in a multiplex bronze table. You'll also validate and quarantine these records before loading them into a silver table.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Describe how filters are applied to streaming jobs\n",
    "- Use built-in functions to flatten nested JSON data\n",
    "- Parse and save binary-encoded strings to native types\n",
    "- Describe and implement a quarantine table\n",
    "\n",
    "## Hint\n",
    "\n",
    "Each step of this lab has you define one additional table in the pipeline. Work on this lab one step at a time. Once you think you have a step implemented, run the pipeline to verify whether the table has the correct results. If not, update the code in this notebook. Then in the dp Pipeline interface, select only the table you're working on for a refresh, and perform a **full refresh** on that table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de4bbedc-c2ee-4028-9cde-53d9f1b51c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bec9ba2d-e084-424e-91dd-e52a13aaf9d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Step 1: Stream Workouts from Multiplex Bronze\n",
    "\n",
    "Stream records for the **`workout`** topic from the multiplex bronze table to create a **`workouts_bronze`** table.\n",
    "1. Start a stream against the **`bronze`** table\n",
    "1. Filter all records by **`topic = 'workout'`**\n",
    "1. Parse and flatten JSON fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15f7f6e7-84c1-46c4-84d6-32866bd93463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "workouts_schema = \"user_id INT, workout_id INT, timestamp FLOAT, action STRING, session_id INT\"\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9613703e-fc6a-4c18-beac-c15a1db612ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "workouts_schema = \"user_id INT, workout_id INT, timestamp FLOAT, action STRING, session_id INT\"\n",
    "\n",
    "@dp.table(\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def workouts_bronze():\n",
    "    return (\n",
    "        dp.read_stream(\"bronze\")\n",
    "          .filter(\"topic = 'workout'\")\n",
    "          .select(F.from_json(F.col(\"value\").cast(\"string\"), workouts_schema).alias(\"v\"))\n",
    "          .select(\"v.*\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59bff055-9d46-4ce1-addf-0ddbbe40953e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 2: Promote Workouts to Silver\n",
    "\n",
    "Process workouts bronze data into a **`workouts_silver`** table with the following schema.\n",
    "\n",
    "| field | type |\n",
    "| --- | --- |\n",
    "| user_id | INT | \n",
    "| workout_id | INT | \n",
    "| time | TIMESTAMP |\n",
    "| action | STRING |\n",
    "| session_id | INT | \n",
    "\n",
    "Validate and promote records from **`workouts_bronze`** to silver by implementing a **`workouts_silver`** table.\n",
    "1. Check that **`user_id`** and **`workout_id`** fields are not null\n",
    "1. Cast **`timestamp`** to timestamp field named **`time`**\n",
    "1. Deduplicate on **`user_id`** and **`time`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1391a859-b535-4c94-af00-e2e619531752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rules = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b243f585-28fb-4373-b922-3fdd42d112ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "rules = {\n",
    "  \"valid_user_id\": \"user_id IS NOT NULL\",\n",
    "  \"valid_workout_id\": \"workout_id IS NOT NULL\"\n",
    "}\n",
    "\n",
    "@dp.table(\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "@dp.expect_all_or_drop(rules)\n",
    "def workouts_silver():\n",
    "    return (\n",
    "        dp.read_stream(\"workouts_bronze\")\n",
    "          .select(\"user_id\", \"workout_id\", F.col(\"timestamp\").cast(\"timestamp\").alias(\"time\"), \"action\", \"session_id\")\n",
    "          .withWatermark(\"time\", \"30 seconds\")\n",
    "          .dropDuplicates([\"user_id\", \"time\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de8ffe27-c600-4658-b6bf-15624d35fc77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 3: Quarantine Invalid Records\n",
    "\n",
    "Implement a **`workouts_quarantine`** table for invalid records from **`workouts_bronze`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53397925-c7ab-450e-9114-6514c26e3f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "quarantine_rules = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c3f5af7-ebd1-4dfc-b404-297c6bb52f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "quarantine_rules = {\"invalid_record\": f\"NOT({' AND '.join(rules.values())})\"}\n",
    "@dp.table\n",
    "@dp.expect_all_or_drop(quarantine_rules)\n",
    "def workouts_quarantine():\n",
    "    return (\n",
    "        dp.read_stream(\"workouts_bronze\")    \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f3fea9-aef4-4e3e-9ed8-ed1683b4f586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "SDLT 2.4.1L - Streaming ETL Lab",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
