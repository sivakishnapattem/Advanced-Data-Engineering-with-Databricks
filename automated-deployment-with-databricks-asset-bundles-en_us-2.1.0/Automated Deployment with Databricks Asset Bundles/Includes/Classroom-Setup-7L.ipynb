{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56993333-8595-436a-ab1d-e6243a71eddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Classroom-Setup-Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faac60f7-92c6-4ed9-86f8-c7072509fd49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Create the DA keys for the user's catalogs\n",
    "DA.create_DA_keys()\n",
    "\n",
    "## Display the course catalog and schema name for the user.\n",
    "DA.display_config_values(\n",
    "  [\n",
    "    ('DEV catalog reference: DA.catalog_dev', DA.catalog_dev),\n",
    "    ('STAGE catalog reference: DA.catalog_stage', DA.catalog_stage),\n",
    "    ('PROD catalog reference: DA.catalog_prod', DA.catalog_prod)\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e73ef829-6954-40ce-a4af-00b8eb6455ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install mlflow==2.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3b7a269-13a4-4ef4-a4a6-fad06034f205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, log, pow, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class MLModelHandler:\n",
    "    def __init__(self):\n",
    "        self.catalog_name = f'{DA.catalog_name}'\n",
    "        self.schema_name = 'default'\n",
    "        self.username = DA.username\n",
    "        self.env = '_1_dev'\n",
    "        self.csv_file_path = f'/Volumes/{DA.catalog_name}_3_prod/default/health/2025-01-02_health.csv'\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        if self.model_existence_check():\n",
    "            print(\"Model already exists in dev and stage. Pipeline did not run.\")\n",
    "            return None\n",
    "        else: \n",
    "            cleaned_df = self.clean_raw_data()\n",
    "            transformed_df = self.transform_cleaned_data(cleaned_df)\n",
    "            feature_df, target_df = self.create_features(transformed_df)\n",
    "            train_data = self.create_training_data(feature_df, target_df)\n",
    "            self.train_and_register_model(train_data)\n",
    "            self.env = '_2_stage'\n",
    "            self.train_and_register_model(train_data)\n",
    "    \n",
    "    def clean_raw_data(self):\n",
    "        raw_data = (\n",
    "            spark.read\n",
    "            .option('header', True)\n",
    "            .option('inferSchema', True)\n",
    "            .csv(self.csv_file_path)\n",
    "        )\n",
    "        numeric_columns = [col_name for col_name in raw_data.columns \n",
    "                           if raw_data.select(col_name).schema[0].dataType.typeName() in ['int', 'double']]\n",
    "        \n",
    "        for column in numeric_columns:\n",
    "            mean_value = raw_data.select(mean(col(column)).alias(\"mean\")).first()[\"mean\"]\n",
    "            raw_data = raw_data.fillna({column: mean_value})\n",
    "        \n",
    "        return raw_data.na.drop()\n",
    "    \n",
    "    def transform_cleaned_data(self, cleaned_df):\n",
    "        transformed_df = (\n",
    "            cleaned_df\n",
    "            .withColumn(\"log_BMI\", log(col(\"BMI\") + 1))\n",
    "            .withColumn(\"log_Age\", log(col(\"Age\") + 1))\n",
    "            .withColumn(\"BMI_squared\", pow(col(\"BMI\"), 2))\n",
    "            .drop(\"PII\", \"date\")\n",
    "            .na.drop()\n",
    "        )\n",
    "        return transformed_df\n",
    "    \n",
    "    def create_features(self, transformed_df):\n",
    "        target_df = transformed_df.select(\"ID\", \"Diabetes_binary\")\n",
    "        feature_df = transformed_df.drop(\"Diabetes_binary\")\n",
    "        feature_df = (\n",
    "            feature_df\n",
    "            .withColumn(\"HighBP\", when(col(\"HighBP\") == \"1.0\", 1).otherwise(0))\n",
    "            .withColumn(\"Age\", col(\"Age\").cast(\"int\"))\n",
    "        )\n",
    "        return feature_df, target_df\n",
    "    \n",
    "    def create_training_data(self, feature_df, target_df):\n",
    "        input_cols = [col_name for col_name in feature_df.columns if col_name != \"ID\"]\n",
    "        assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "        feature_data = assembler.transform(feature_df)\n",
    "        sdf = feature_data.join(target_df, on=\"ID\")\n",
    "        train_data, test_data = sdf.randomSplit([0.8, 0.2], seed=42)\n",
    "        return train_data\n",
    "    \n",
    "    def model_existence_check(self):\n",
    "        mlflow.set_registry_uri(\"databricks-uc\")\n",
    "        client = MlflowClient()\n",
    "        model_env_name = f\"diabetes_model_dev\"\n",
    "        registry_model_dev = f\"{self.catalog_name}{self.env}.{self.schema_name}.{model_env_name}\" # If dev exists, stage will also exist \n",
    "        existing_versions = client.search_model_versions(f\"name = '{registry_model_dev}'\")\n",
    "        print(f\"Checking to see if model exists in {self.catalog_name}{self.env}...\")\n",
    "        if existing_versions:\n",
    "            latest_ver = max(int(v.version) for v in existing_versions)\n",
    "            model_uri   = f\"models:/{registry_model_dev}/{latest_ver}\"\n",
    "            print(f\"Model already exists: {model_uri}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"No versions found in {self.catalog_name}{self.env}{self.env}; you can train and register a new model.\")\n",
    "            return False\n",
    "\n",
    "    \n",
    "    def train_and_register_model(self, train_data):\n",
    "        # Train & log new model\n",
    "        model_base = f\"{self.catalog_name}{self.env}.{self.schema_name}.diabetes_model_dev\"\n",
    "        print(f\"No existing model found; training & registering diabetes_model in {self.catalog_name}{self.env}\")\n",
    "        with mlflow.start_run():\n",
    "            rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"Diabetes_binary\")\n",
    "            model = rf.fit(train_data)\n",
    "            # infer signature on trainâ†’predict\n",
    "            sig = infer_signature(\n",
    "                train_data.select(\"features\").toPandas(),\n",
    "                model.transform(train_data).select(\"prediction\").toPandas()\n",
    "            )\n",
    "\n",
    "            # log & register\n",
    "            mlflow.log_param(\"environment\", self.env)\n",
    "            mlflow.spark.log_model(\n",
    "                spark_model = model,\n",
    "                artifact_path = \"model\",\n",
    "                registered_model_name = model_base,\n",
    "                signature = sig\n",
    "            )\n",
    "        print(f\"Model registered successfully for environment: {self.catalog_name}{self.env}\")\n",
    "\n",
    "# Run the pipeline\n",
    "handler = MLModelHandler()\n",
    "handler.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "852668fb-85a7-4d72-b76b-8c1930018b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Classroom-Setup-Common-Install-CLI"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-7L",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
