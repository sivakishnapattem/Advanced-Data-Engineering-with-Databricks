{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a13f6b9-45d0-4d29-ae73-752b06a592e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../Includes/_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f415c43e-8c48-493f-a9e5-d4fad54bb0cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_DA_keys(self): \n",
    "    '''\n",
    "    Create the DA references to the dev, prod and stage catalogs for the user.\n",
    "    '''\n",
    "    print('Set DA dynamic references to the dev, stage and prod catalogs.\\n')\n",
    "    setattr(DA, f'catalog_dev', f'{self.catalog_name}_1_dev')\n",
    "    setattr(DA, f'catalog_stage', f'{self.catalog_name}_2_stage')\n",
    "    setattr(DA, f'catalog_prod', f'{self.catalog_name}_3_prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd5eb3a-de0a-4a98-b226-c4f91d8d3894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_if_catalogs_are_created(check_catalogs: list[str]):\n",
    "    '''\n",
    "    Search for the dev, stage, prod catalogs by default. Return error if those don't exist.\n",
    "    '''\n",
    "\n",
    "    list_of_catalogs = spark.sql('SHOW CATALOGS')\n",
    "    end_of_catalog_names = set(list_of_catalogs.toPandas().catalog.str.split('_').str[-1].to_list())\n",
    "    \n",
    "    # Convert check_catalogs to a set\n",
    "    check_catalogs_set = set(check_catalogs)\n",
    "    \n",
    "    # Check if all items are in the predefined items set\n",
    "    missing_items = check_catalogs_set - end_of_catalog_names\n",
    "    \n",
    "    if missing_items:\n",
    "        # If there are any missing items, raise an error\n",
    "        raise ValueError(f\"Necessary catalogs do not exist. Please run the 0 - REQUIRED - Course Setup and Authentication notebook to setup your environment.\")\n",
    "    \n",
    "    # If all items are found, return True\n",
    "    print('Catalog check for the labs passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc6c1274-bb3c-413a-8542-685ded1ff58b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def delete_source_files(self, source_files):\n",
    "        \"\"\"\n",
    "        Deletes all files in the specified source volume.\n",
    "\n",
    "        This function iterates through all the files in the given volume,\n",
    "        deletes them, and prints the name of each file being deleted.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        source_files : str, optional\n",
    "            The path to the volume containing the files to delete. \n",
    "            Use the {DA.paths.working_dir} to dynamically navigate to the user's volume location in dbacademy/ops/vocareumlab@name:\n",
    "                Example: DA.paths.working_dir = /Volumes/dbacademy/ops/vocareumlab@name\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        None\n",
    "            This function does not return any value. It performs file deletion as a side effect and prints all files that it deletes.\n",
    "\n",
    "        Example:\n",
    "        --------\n",
    "        delete_source_files(f'{DA.paths.working_dir}/pii/stream_source/user_reg')\n",
    "        \"\"\"\n",
    "        \n",
    "        import os\n",
    "        \n",
    "        print(f'\\nSearching for files in {source_files} volume to delete prior to creating files...')\n",
    "        if os.path.exists(source_files):\n",
    "            list_of_files = sorted(os.listdir(source_files))\n",
    "        else:\n",
    "            list_of_files = None\n",
    "\n",
    "        if not list_of_files:  # Checks if the list is empty.\n",
    "            print(f\"No files found in {source_files}.\\n\")\n",
    "        else:\n",
    "            for file in list_of_files:\n",
    "                file_to_delete = source_files + '/' + file\n",
    "                print(f'Deleting file: {file_to_delete}')\n",
    "                dbutils.fs.rm(file_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8064984-3433-4079-a0c1-9fa066f50b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DeclarativePipelineCreator:\n",
    "    \"\"\"\n",
    "    A class to create a Lakeflow Declarative DLT pipeline using the Databricks REST API.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    pipeline_name : str\n",
    "        Name of the pipeline to be created.\n",
    "    root_path_folder_name : str\n",
    "        The folder containing the pipeline code relative to current working directory.\n",
    "    source_folder_names : list\n",
    "        List of subfolders inside the root path containing source notebooks or scripts.\n",
    "    catalog_name : str\n",
    "        The catalog where the pipeline tables will be stored.\n",
    "    schema_name : str\n",
    "        The schema (aka database) under the catalog.\n",
    "    serverless : bool\n",
    "        Whether to use serverless compute.\n",
    "    configuration : dict\n",
    "        Optional key-value configurations passed to the pipeline.\n",
    "    continuous : bool\n",
    "        If True, enables continuous mode (streaming).\n",
    "    photon : bool\n",
    "        Whether to use Photon execution engine.\n",
    "    channel : str\n",
    "        The DLT release channel to use (e.g., \"PREVIEW\", \"CURRENT\").\n",
    "    development : bool\n",
    "        Whether to run the pipeline in development mode.\n",
    "    pipeline_type : str\n",
    "        Type of pipeline (e.g., 'WORKSPACE').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pipeline_name: str,\n",
    "                 root_path_folder_name: str,\n",
    "                 catalog_name: str,\n",
    "                 schema_name: str,\n",
    "                 source_folder_names: list = None,\n",
    "                 serverless: bool = True,\n",
    "                 configuration: dict = None,\n",
    "                 continuous: bool = False,\n",
    "                 photon: bool = True,\n",
    "                 channel: str = 'PREVIEW',\n",
    "                 development: bool = True,\n",
    "                 pipeline_type: str = 'WORKSPACE'):\n",
    "\n",
    "        # Assign all input arguments to instance attributes\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.root_path_folder_name = root_path_folder_name\n",
    "        self.source_folder_names = source_folder_names or []\n",
    "        self.catalog_name = catalog_name\n",
    "        self.schema_name = schema_name\n",
    "        self.serverless = serverless\n",
    "        self.configuration = configuration or {}\n",
    "        self.continuous = continuous\n",
    "        self.photon = photon\n",
    "        self.channel = channel\n",
    "        self.development = development\n",
    "        self.pipeline_type = pipeline_type\n",
    "\n",
    "        # Instantiate the WorkspaceClient to communicate with Databricks REST API\n",
    "        self.workspace = WorkspaceClient()\n",
    "        self.pipeline_body = {}\n",
    "\n",
    "    def _check_pipeline_exists(self):\n",
    "        \"\"\"\n",
    "        Checks if a pipeline with the same name already exists.\n",
    "        Raises:\n",
    "            ValueError if the pipeline already exists.\n",
    "        \"\"\"\n",
    "        for pipeline in self.workspace.pipelines.list_pipelines():\n",
    "            if pipeline.name == self.pipeline_name:\n",
    "                raise ValueError(\n",
    "                    f\"Lakeflow Declarative Pipeline name '{self.pipeline_name}' already exists. \"\n",
    "                    \"Please delete the pipeline using the UI and rerun to recreate.\"\n",
    "                )\n",
    "\n",
    "    def _build_pipeline_body(self):\n",
    "        \"\"\"\n",
    "        Constructs the body of the pipeline creation request based on class attributes.\n",
    "        \"\"\"\n",
    "        # Get current working directory\n",
    "        cwd = os.getcwd()\n",
    "\n",
    "        # Source folder is not in current working directory, so go up two level (Only for this course)\n",
    "        main_course_folder = os.path.dirname(os.path.dirname(cwd))\n",
    "\n",
    "        # Create full path to root folder\n",
    "        root_path_folder = os.path.join('/', main_course_folder, self.root_path_folder_name)\n",
    "\n",
    "        # Convert source folder names into glob pattern paths for the DLT pipeline\n",
    "        source_paths = [os.path.join(main_course_folder, folder) for folder in self.source_folder_names]\n",
    "        libraries = [{'glob': {'include': path}} for path in source_paths]\n",
    "\n",
    "        # Build dictionary to be sent in the API request\n",
    "        self.pipeline_body = {\n",
    "            'name': self.pipeline_name,\n",
    "            'pipeline_type': self.pipeline_type,\n",
    "            'root_path': root_path_folder,\n",
    "            'libraries': libraries,\n",
    "            'catalog': self.catalog_name,\n",
    "            'schema': self.schema_name,\n",
    "            'serverless': self.serverless,\n",
    "            'configuration': self.configuration,\n",
    "            'continuous': self.continuous,\n",
    "            'photon': self.photon,\n",
    "            'channel': self.channel,\n",
    "            'development': self.development\n",
    "        }\n",
    "\n",
    "    def create_pipeline(self):\n",
    "        \"\"\"\n",
    "        Creates the pipeline on Databricks using the defined attributes.\n",
    "\n",
    "        Returns:\n",
    "            dict: The response from the Databricks API after creating the pipeline.\n",
    "        \"\"\"\n",
    "        # Check for name conflicts\n",
    "        self._check_pipeline_exists()\n",
    "\n",
    "        # Build the body of the API request and creates self.pipeline_body variable\n",
    "        self._build_pipeline_body()\n",
    "\n",
    "        # Display information to user\n",
    "        print(f\"Creating the Lakeflow Declarative Pipeline '{self.pipeline_name}'...\")\n",
    "        print(f\"Root folder path: {self.pipeline_body['root_path']}\")\n",
    "        print(f\"Source folder path(s): {self.pipeline_body['libraries']}\")\n",
    "\n",
    "        # Make the API call\n",
    "        self.response = self.workspace.api_client.do('POST', '/api/2.0/pipelines', body=self.pipeline_body)\n",
    "\n",
    "        # Notify of completion\n",
    "        print(f\"\\nLakeflow Declarative Pipeline Creation '{self.pipeline_name}' Complete!\")\n",
    "\n",
    "        return self.response\n",
    "\n",
    "    def get_pipeline_id(self):\n",
    "        \"\"\"\n",
    "        Returns the ID of the created pipeline.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'response'):\n",
    "            raise RuntimeError(\"Pipeline has not been created yet. Call create_pipeline() first.\")\n",
    "\n",
    "        return self.response.get(\"pipeline_id\")\n",
    "\n",
    "    def start_pipeline(self):\n",
    "        '''\n",
    "        Starts the pipeline using the attribute set from the generate_pipeline() method.\n",
    "        '''\n",
    "        print('Started the pipeline run. Navigate to Jobs and Pipelines to view the pipeline.')\n",
    "        self.workspace.pipelines.start_update(self.get_pipeline_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b95fb960-0f37-4a70-861b-9adcbc613c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks.sdk.service import jobs, pipelines\n",
    "from databricks.sdk import WorkspaceClient  \n",
    "\n",
    "class DAJobConfig:\n",
    "    '''\n",
    "    Example\n",
    "    ------------\n",
    "    job_tasks = [\n",
    "        {\n",
    "            'task_name': 'create_table',\n",
    "            'notebook_path': '/01 - Simple DAB/create_table',\n",
    "            'depends_on': None\n",
    "        },\n",
    "        {\n",
    "            'task_name': 'create_table1',\n",
    "            'notebook_path': '/01 - Simple DAB/other_table2',\n",
    "            'depends_on': [{'task_key': 'create_table'}]\n",
    "        },\n",
    "        {\n",
    "            'task_name': 'create_table3',\n",
    "            'notebook_path': '/01 - Simple DAB/other_table2',\n",
    "            'depends_on': [{'task_key': 'create_table'},{'task_key': 'create_table1'}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "    myjob = DAJobConfig(job_name='test3',\n",
    "                        job_tasks=job_tasks,\n",
    "                        job_parameters=[\n",
    "                            {'name':'target', 'default':'dev'},\n",
    "                            {'name':'catalog_name', 'default':'test'}\n",
    "                        ])\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 job_name: str,\n",
    "                 job_tasks: list[dict],\n",
    "                 job_parameters: list[dict]):\n",
    "    \n",
    "        self.job_name = job_name\n",
    "        self.job_tasks = job_tasks\n",
    "        self.job_parameters = job_parameters\n",
    "        \n",
    "        ## Connect the Workspace\n",
    "        self.w = self.get_workspace_client()\n",
    "\n",
    "        ## Execute methods\n",
    "        self.check_for_duplicate_job_name(check_job_name=self.job_name)\n",
    "        print(f'Job name is unique. Creating the job {self.job_name}...')\n",
    "\n",
    "        self.course_path = self.get_path_one_folder_back()\n",
    "        self.list_job_tasks = self.create_job_tasks()\n",
    "\n",
    "        self.create_job(job_tasks = self.list_job_tasks)\n",
    "\n",
    "\n",
    "    ## Get Workspace client\n",
    "    def get_workspace_client(self):\n",
    "        \"\"\"\n",
    "        Establishes and returns a WorkspaceClient instance for interacting with the Databricks API.\n",
    "        This is set when the object is created within self.w\n",
    "\n",
    "        Returns:\n",
    "            WorkspaceClient: A client instance to interact with the Databricks workspace.\n",
    "        \"\"\"\n",
    "        w = WorkspaceClient()\n",
    "        return w\n",
    "\n",
    "\n",
    "    # Check if the job name already exists, return error if it does.\n",
    "    def check_for_duplicate_job_name(self, check_job_name: str):\n",
    "        for job in self.w.jobs.list():\n",
    "            if job.settings.name == check_job_name:\n",
    "                test_job_name = False\n",
    "                assert test_job_name, f'You already have a job with the same name. Please manually delete the job {self.job_name}'                \n",
    "\n",
    "\n",
    "    ## Store the path of one folder one folder back\n",
    "    def get_path_one_folder_back(self):\n",
    "        current_path = os.path.dirname(os.getcwd())\n",
    "        print(f'Using the following path to reference the notebooks: {current_path}/.')\n",
    "        return current_path\n",
    "\n",
    "\n",
    "    ## Create the job tasks\n",
    "    def create_job_tasks(self):\n",
    "        all_job_tasks = []\n",
    "        for task in job_tasks:\n",
    "            if task.get('notebook_path', False) != False:\n",
    "\n",
    "                ## Create a list of jobs.TaskDependencies\n",
    "                task_dependencies = [jobs.TaskDependency(task_key=depend_task['task_key']) for depend_task in task['depends_on']] if task['depends_on'] else None\n",
    "\n",
    "                ## Create the task\n",
    "                job_task_notebook = jobs.Task(task_key=task['task_name'],\n",
    "                                              notebook_task=jobs.NotebookTask(notebook_path=self.course_path+task['notebook_path']),\n",
    "                                              depends_on=task_dependencies,\n",
    "                                              timeout_seconds=0)\n",
    "                all_job_tasks.append(job_task_notebook)\n",
    "\n",
    "            elif task.get('pipeline_task', False) != False:\n",
    "                job_task_dlt = jobs.Task(task_key=task['task_name'],\n",
    "                                         pipeline_task=jobs.PipelineTask(pipeline_id=task['pipeline_id'], full_refresh=True),\n",
    "                                         timeout_seconds=0)\n",
    "                all_job_tasks.append(job_task_info)\n",
    "\n",
    "        return all_job_tasks\n",
    "    \n",
    "\n",
    "    def set_job_parameters(self, parameters: dict):\n",
    "\n",
    "        job_params_list = []\n",
    "        for param in self.job_parameters:\n",
    "            job_parameter = jobs.JobParameterDefinition(name=param['name'], default=param['default'])\n",
    "            job_params_list.append(job_parameter)\n",
    "\n",
    "        return job_params_list\n",
    "    \n",
    "\n",
    "    ## Create final job\n",
    "    def create_job(self, job_tasks: list[jobs.Task]):\n",
    "        created_job = self.w.jobs.create(\n",
    "                name=self.job_name,\n",
    "                tasks=job_tasks,\n",
    "                parameters = self.set_job_parameters(self.job_parameters)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc21d3fb-304d-458c-9313-36eb3c6fff13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def load_credentials(self):\n",
    "    from pathlib import Path\n",
    "    import configparser\n",
    "    c = configparser.ConfigParser()\n",
    "\n",
    "    folder_name = f'var_{DA.catalog_name}'\n",
    "\n",
    "    # Get the current directory and one directory back to search for the credentials.cfg file.\n",
    "    current_path = Path.cwd()\n",
    "    go_back_path = current_path.parents[0]\n",
    "\n",
    "    find_current_path_cfg_file  = current_path / f'{folder_name}/credentials.cfg'\n",
    "    find_back_path_cfg_file  = go_back_path / f'{folder_name}/credentials.cfg'\n",
    "\n",
    "    ## Search for the credentials.cfg file. If not found it does not exist.\n",
    "    if os.path.exists(find_current_path_cfg_file):\n",
    "        print(f'Found credentials.cfg in {find_current_path_cfg_file}.')\n",
    "        c.read(filenames=find_current_path_cfg_file)\n",
    "    elif os.path.exists(find_back_path_cfg_file):\n",
    "        print(f'Found credentials.cfg in {find_back_path_cfg_file}.')\n",
    "        c.read(filenames=find_back_path_cfg_file)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        token = c.get('DEFAULT', 'db_token')\n",
    "        host = c.get('DEFAULT', 'db_instance')\n",
    "        os.environ[\"DATABRICKS_HOST\"] = host\n",
    "        os.environ[\"DATABRICKS_TOKEN\"] = token\n",
    "    except:\n",
    "        token = ''\n",
    "        host = ''\n",
    "\n",
    "    return token, host    \n",
    "\n",
    "@DBAcademyHelper.add_method\n",
    "def get_credentials(self):\n",
    "\n",
    "    ## Get Databricks Lab URL value to use in the step below.\n",
    "    lab_databricks_url = f'https://{spark.conf.get(\"spark.databricks.workspaceUrl\")}/'\n",
    "    \n",
    "    import ipywidgets as widgets\n",
    "\n",
    "    (current_token, current_host) = self.load_credentials()\n",
    "    current_host = lab_databricks_url\n",
    "    \n",
    "    @widgets.interact(host=widgets.Text(description='Host:',\n",
    "                                        placeholder='Paste workspace URL here',\n",
    "                                        value = current_host,\n",
    "                                        continuous_update=False),\n",
    "                      token=widgets.Password(description='Token:',\n",
    "                                             placeholder='Paste PAT here',\n",
    "                                             value = current_token,\n",
    "                                             continuous_update=False)\n",
    "    )\n",
    "    def _f(host='', token=''):\n",
    "        from urllib.parse import urlparse,urlunsplit\n",
    "\n",
    "        u = urlparse(host)\n",
    "        host = urlunsplit((u.scheme, u.netloc, '', '', ''))\n",
    "\n",
    "        if host and token:\n",
    "            os.environ[\"DATABRICKS_HOST\"] = host\n",
    "            os.environ[\"DATABRICKS_TOKEN\"] = token\n",
    "\n",
    "            contents = f\"\"\"\n",
    "[DEFAULT]\n",
    "db_token = {token}\n",
    "db_instance = {host}\n",
    "            \"\"\"\n",
    "            make_folder = f'var_{DA.catalog_name}'\n",
    "            os.makedirs(make_folder, exist_ok=True)\n",
    "            with open(f\"{make_folder}/credentials.cfg\", \"w\") as f:\n",
    "                print(f\"Credentials stored ({f.write(contents)} bytes written).\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5499cef1-4901-44f3-80d6-2ee7185ab4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_taxi_dev_data():\n",
    "    spark.sql(f'''\n",
    "        CREATE OR REPLACE TABLE {DA.catalog_dev}.default.nyctaxi_raw AS\n",
    "        SELECT *\n",
    "        FROM samples.nyctaxi.trips\n",
    "        LIMIT 100\n",
    "    ''')\n",
    "    print(f'Created the nyctaxi_dev table in your dev catalog: {DA.catalog_dev}!')\n",
    "        \n",
    "def create_taxi_prod_data():\n",
    "    spark.sql(f'''\n",
    "        CREATE OR REPLACE TABLE {DA.catalog_prod}.default.nyctaxi_raw AS\n",
    "        SELECT *\n",
    "        FROM samples.nyctaxi.trips\n",
    "    ''')\n",
    "    print(f'Created the nyctaxi_prod table in your dev catalog: {DA.catalog_prod}!')\n",
    "\n",
    "\n",
    "def check_nyctaxi_bronze_table(user_catalog: str, total_count: int):\n",
    "    total_rows_in_table = spark.sql(f'''\n",
    "        SELECT count(*)\n",
    "        FROM {user_catalog}.default.nyctaxi_bronze\n",
    "    ''').collect()\n",
    "\n",
    "    assert total_rows_in_table[0][0] == total_count, 'The bronze table was not created successfully'\n",
    "    print(f'The nyctaxi_bronze table has was created successfully from your DAB deployment!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf49e5c7-14d6-493b-b297-8ab7af3f08b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def del_table(catalog, schema, table):\n",
    "    print(f'Deleting the table {catalog}.{schema}.{table} if it exists.')\n",
    "    spark.sql(f'DROP TABLE IF EXISTS {catalog}.{schema}.{table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a144e58-6a34-40d4-8d26-2aa2e8aec7c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA = DBAcademyHelper()\n",
    "DA.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df37ea15-2492-4816-ba24-b0f101263971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Initialize the next lesson and load credentials\n",
    "# LESSON = \"using_cli\"\n",
    "# # Load Credentials\n",
    "# (db_token, db_instance) = DA.load_credentials()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-Common",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
