{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34013efc-2273-4bbc-8d4e-26dd3da58485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Inference on the Silver Table\n",
    "\n",
    "The purpose of this notebook is to perform inference on the streaming table stored in the `dev` catalog. The model being used here was created during the classroom setup script for the associated lab. \n",
    "\n",
    "## Steps:\n",
    "1. Read in the streaming table and perform some data transformations to prepare it to be used as an input for our model. \n",
    "1. Load a pre-trained model from Unity Catalog. This is located in the staging catalog. \n",
    "1. Make a prediction on the transformed streaming table on the first 2 rows to validate our silver layer for the ML team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dce4531-02a6-4ef5-b830-c8be2bf3efa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parameterize the notebook for our workflow and passing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25963f26-f5ed-4d30-8c1a-4b0fa8e7dfe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_model_name = dbutils.widgets.get(\"base_model_name\") # Set to \"diabetes_model_dev\" in the parameterization of the notebook in the workflow \n",
    "silver_table_name = dbutils.widgets.get('silver_table_name')# Set to \"<username>_1_dev.default.health_silver\"\n",
    "catalog_name = dbutils.widgets.get('catalog_name')# Set to \"<username>_1_dev.default.diabetes_model_dev\"\n",
    "print(base_model_name)\n",
    "print(silver_table_name)\n",
    "print(catalog_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7387ab69-7440-4e11-a667-856d6d888c0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Read and transform the silver-layer streaming table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd3cfb4b-1202-4a46-944a-cadf69b7fe51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow \n",
    "\n",
    "from pyspark.sql.functions import col, log, pow\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Create a function to transform columns to be used for inference on streaming data\n",
    "def create_streaming_features(silver_table: str) -> DataFrame:\n",
    "    # Read streaming data\n",
    "    stream_df = (\n",
    "        spark.read\n",
    "        .table(silver_table)  # Assumes a streaming table is registered in Unity Catalog\n",
    "    )\n",
    "\n",
    "    # Transform the data to include required computed features\n",
    "    transformed_stream_df = (\n",
    "        stream_df\n",
    "        .withColumn(\"log_BMI\", log(col(\"BMI\") + 1))\n",
    "        .withColumn(\"log_Age\", log(col(\"Age\") + 1))\n",
    "        .withColumn(\"BMI_squared\", pow(col(\"BMI\"), 2))\n",
    "        .drop(\"PII\", \"date\")\n",
    "        .na.drop()\n",
    "    )\n",
    "\n",
    "    # Ensure features are transformed as done during training\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"HighCholest\", \"HighBP\", \"BMI\", \"Age\", \"Education\", \"income\", \"log_BMI\", \"log_Age\", \"BMI_squared\"], \n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    stream_features = assembler.transform(transformed_stream_df)\n",
    "    print(\"Silver table successfully transformed!\")\n",
    "    return stream_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2f8fdfc-4338-4fc5-a838-8f1678281291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Load the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc9dce50-79fe-4ab2-b8e9-08c406d8d3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "def load_ml_model(env: str) -> PipelineModel:\n",
    "    model_base_name = base_model_name\n",
    "    full_model_name = f\"{catalog_name}.default.{model_base_name}\"\n",
    "\n",
    "    # Retrieve the latest version of the model\n",
    "    client = MlflowClient()\n",
    "    model_version_infos = client.search_model_versions(f\"name = '{full_model_name}'\")\n",
    "\n",
    "    if model_version_infos:\n",
    "        latest_version = max([int(info.version) for info in model_version_infos])\n",
    "        model_uri = f\"models:/{full_model_name}/{latest_version}\"\n",
    "        print(f\"Found model {full_model_name} in {env}\")\n",
    "        print(f\"Loading model version {latest_version} from MLflow...\")\n",
    "        return mlflow.spark.load_model(model_uri)  # Ensure correct model loading\n",
    "    else:\n",
    "        raise ValueError(f\"No registered versions of {full_model_name} found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2db69fe-ef43-4f11-8023-cd8ba7152997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Make a prediction on the transformed streaming table and output the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c803445-0f63-4ee6-b522-85ae2f1dc1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "\n",
    "\n",
    "def make_prediction(sample_df: DataFrame, loaded_model: PipelineModel):\n",
    "    \"\"\"\n",
    "    Applies feature transformation and runs inference using the loaded model.\n",
    "    \"\"\"\n",
    "    feature_cols = [\n",
    "        \"HighCholest\",  \n",
    "        \"HighBP\",\n",
    "        \"BMI\",\n",
    "        \"Age\",\n",
    "        \"Education\",\n",
    "        \"income\",\n",
    "        \"log_BMI\",\n",
    "        \"log_Age\",\n",
    "        \"BMI_squared\"\n",
    "    ]\n",
    "\n",
    "    # Ensure all required columns exist\n",
    "    missing_cols = [col for col in feature_cols if col not in sample_df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in input DataFrame: {missing_cols}\")\n",
    "\n",
    "    # Check if 'features' column already exists\n",
    "    if \"features\" not in sample_df.columns:\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        sample_df = assembler.transform(sample_df)\n",
    "\n",
    "    print(\"Inferenceing sample silver table data...\")\n",
    "    # Perform inference\n",
    "    predictions = loaded_model.transform(sample_df)\n",
    "    \n",
    "    return predictions.select(\"prediction\")  # Return only predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dde7d2fe-001d-4650-b3fb-a7c36db49556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df = create_streaming_features(silver_table_name)\n",
    "loaded_model = load_ml_model('dev')\n",
    "sample_prediction = make_prediction(silver_df, loaded_model)\n",
    "display(sample_prediction)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Inference",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
