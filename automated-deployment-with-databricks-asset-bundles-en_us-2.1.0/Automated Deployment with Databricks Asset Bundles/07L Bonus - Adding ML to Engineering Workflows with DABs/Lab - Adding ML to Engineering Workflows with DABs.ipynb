{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a9e418c-8c62-48ab-9085-f98b1373e46d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f937545-950a-4d2e-b492-f9b39a5b91a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 07L - Adding ML to Engineering Workflows with DABs\n",
    "\n",
    "### Estimated Duration: 25-30 minutes\n",
    "\n",
    "In this lab, you will\n",
    "- Create and maintain variable configurations for data assets with DABs.\n",
    "- Understand and modify bundle YAML configuration files.\n",
    "- Use the Databricks CLI with Notebooks to validate and deploy DABs with a ML asset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2cdbac2-3272-4955-91d3-0c6fc10c36c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "  - In the drop-down, select **More**.\n",
    "\n",
    "  - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc413ec2-4cb5-42c0-bb50-eae7a30d1265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course.\n",
    "\n",
    "**NOTE:** This will take 2-3 minutes to setup and create the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ea03054-6b13-4c3d-ac91-06140ba533d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-7L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5091ccb-2d7f-4695-95fd-33722ffb4d4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## IMPORTANT LAB INFORMATION\n",
    "\n",
    "Recall that your credentials are stored in a file when running [0 - REQUIRED - Course Setup and Authentication]($../0 - REQUIRED - Course Setup and Authentication).\n",
    "\n",
    "If you end your lab or your lab session times out, your environment will be reset.\n",
    "\n",
    "If you encounter an error regarding unavailable catalogs or if your Databricks CLI is not authenticated, you will need to rerun the [0 - REQUIRED - Course Setup and Authentication]($../0 - REQUIRED - Course Setup and Authentication) notebook to recreate the catalogs and your Databricks CLI credentials.\n",
    "\n",
    "**Use classic compute to use the CLI through a notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f43df8e-ef25-431d-b2bd-076c4bde56f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SCENARIO\n",
    "\n",
    "Congratulations! You’ve successfully built the bulk of your workflow. The ML team has asked you to ensure your tests meet their requirements for inferencing a model they've deployed in the Dev environment. You don’t need to learn ML—just know how to attach the model to the workflow using the bundle you’ve already built.\n",
    "\n",
    "**Optional task before starting:** For data scientists with ML knowledge, you can inspect the pre-trained model by navigating to experiments. For this demonstration, you don't need to understand the model—your goal is simply to add it to your bundle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6132bc82-24fa-4a2f-9364-e36c60ff1acb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the Databricks CLI command below to confirm the Databricks CLI is authenticated.\n",
    "\n",
    "</br>\n",
    "\n",
    "##### DATABRICKS CLI ERROR TROUBLESHOOTING:\n",
    "  - If you encounter an Databricks CLI authentication error, it means you haven't created the PAT token specified in notebook **0 - REQUIRED - Course Setup and Authentication**. You will need to set up Databricks CLI authentication as shown in that notebook.\n",
    "\n",
    "  - If you encounter the error below, it means your `databricks.yml` file is invalid due to a modification. Even for non-DAB CLI commands, the `databricks.yml` file is still required, as it may contain important authentication details, such as the host and profile, which are utilized by the CLI commands.\n",
    "\n",
    "![CLI Invalid YAML](../Includes/images/databricks_cli_error_invalid_yaml.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26169375-9cf2-4c55-bdbd-0c49e3da3f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "creds = DA.load_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8815da43-3d70-4f7f-ac97-e30344cf7a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "databricks workspace list /Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adae8e8c-c37c-4eca-8464-d5e002deef89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Update **variables.yml**\n",
    "\n",
    "Within the folder where this notebook is located, you will find a folder called [**TODO - Lab DABs Workflows**]($./TODO - Lab DABs Workflow). You will be updating some of the files in this folder to attach a machine learning model to the workflow. You *do not* need to know what this model does. The goal of this exercise is to understand how to attach an additional Unity Catalog asset, which is a registered ML model in this case.\n",
    "\n",
    "### Instructions:\n",
    "1. Navigate to the **src** folder. Here you will find two folders, **dlt_pipelines** and **helpers**, and two notebooks, **Final Visualization** and **Inference**. The notebook we’ll focus on for the lab is **Inference**. Click on it and inspect the cells.\n",
    "\n",
    "2. In a separate tab, navigate to **resources** and click on **variables.yml**. We will need to update this YAML file with some additional variables.\n",
    "\n",
    "3. In the **Inference** notebook, you’ll see some variables being called in the cell under the header **Parameterize the notebook for our workflow and passing variables**—namely:\n",
    "\n",
    "    - `base_model_name` - The name of the model\n",
    "\n",
    "    - `silver_table_name` - The name and location of the silver table **catalog.schema.silver_sample_ml**\n",
    "\n",
    "4. We will need to point these to our bundle YAML files.\n",
    "\n",
    "    - Complete the two new variables, `base_model_name` and `silver_table_name`, in the **variables.yml** file in the section marked **PLEASE ONLY CHANGE THE VARIABLES IN THE FOLLOWING SECTION**.\n",
    "\n",
    "        - To find the default value for `base_model_name`, locate the model in the dev catalog under **Models**.\n",
    "\n",
    "        - The default value for `silver_table_name` should is set to **silver_sample_ml**.\n",
    "\n",
    "4. We will use a cluster for this ML task. Define a third variable called `cluster_id`. You have four options for defining this variable:\n",
    "\n",
    "    - Option 1: Define the lookup variable in username and use `${var.username}` to reference the `username` variable.\n",
    "\n",
    "    - Option 2: Use `lookup` and set the `cluster` value to `${workspace.current_user.userName}`.\n",
    "\n",
    "    - Option 3: Hardcode the default value using the `lookup` method.\n",
    "\n",
    "    - Option 4: Find your cluster ID by navigating to Compute on the left menu, clicking on your cluster, selecting the three vertical dots, and clicking **View JSON**. Copy the cluster ID near the top of the JSON. Alternatively, you can get the cluster ID by running the following code snippet in a new cell: `print(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\"))`. Paste this value for the `default` value of `cluster_id` in the `variables.yml` file.\n",
    "\n",
    "### Summary:\n",
    "By completing the tasks, you should have created three new variables: `base_model_name`, `silver_table_name`, and `cluster_id`. Each variable will have a description and a default value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4638e0ca-e343-4092-b358-bb69493f4a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Update **dabs_workflow_with_ml.job.yml**\n",
    "\n",
    "Now that we’ve updated our **variables.yml** file, let's move on to updating our workflow. We will not be configuring the Lakeflow Declarative pipeline in this step.\n",
    "\n",
    "### Instructions:\n",
    "Navigate to **resources** and open the **job** folder. Here you will find all the tasks previously created. Create a new task for inferencing the ML model with the following constraints:\n",
    "\n",
    "1. The task name can be anything you choose.\n",
    "\n",
    "1. The task must depend on **Health_ETL**.\n",
    "\n",
    "1. Add a key called `existing_cluster_id` and set its value to reference the `cluster_id` variable you created in the previous step.\n",
    "\n",
    "1. Add a `notebook_task` that contains a `notebook_path`, `base_parameters`, and `source`:\n",
    "    - `notebook_path` should reference our `Inference` notebook.\n",
    "\n",
    "    - `base_parameters` should have 3 keys. There are two keys that reference the variables we created earlier: `base_model_name` and `silver_table_name` and one that will reference the dev catalog (_Hint: use a variable that was already pre-configured in the `variables.yml` file_).\n",
    "\n",
    "    - You can also provide a description if desired.\n",
    "\n",
    "**HINT**: Use the existing tasks as templates to help with this step.\n",
    "\n",
    "### Summary:\n",
    "By completing this task, you should have created a new task in your workflow within `dabs_workflow.job.yml` and be ready to validate the bundle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7544c27-3a64-4473-86a8-c82eff7ba988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Bundle Validation and Deployment\n",
    "Now that you have updated your **variables.yml** and **dabs_workflow.job.yml`** files, you are ready to validate your bundle before deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a61d4f6-334b-4411-a5e2-9aab1004e009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use the Databricks CLI to print out a summary of all of the resources defined in the project and the corresponding names which will be generated after deploying the bundle. Note, you will have to `cd` into the bundle folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e8a7bee-a51c-41d5-80ad-780def2d5cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "cd \"./TODO - Lab DABs Workflow\"\n",
    "databricks bundle summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61970982-856b-4938-a1cf-4125bb19b019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use the Databricks CLI to validate the bundle. Note, you will have to `cd` into the bundle folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "064c77e3-97ad-4a73-9e14-08d3f49be92c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "cd \"./TODO - Lab DABs Workflow\"\n",
    "pwd;\n",
    "databricks bundle validate -t development;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce7365f-a56a-426a-92a0-7bbfa1470e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deploy the bundle to the development environment. Note, you will have to `cd` into the bundle folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ebf0775-3e72-48a0-b65b-d1b6c2f2dcd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cd \"./TODO - Lab DABs Workflow\"\n",
    "databricks bundle deploy -t development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0087be32-130a-4e1d-82dd-cae3d98c454c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Run the Job\n",
    "Option 1: Use the UI to run the job and visually watch the tasks kick off. Click on each task to inspect the notebooks or Lakeflow Declarative pipeline. \n",
    "\n",
    "Option 2: Run the job using the CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe2ba947-ecf1-4165-a349-0a795d555ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note: You will need to delete the Lakeflow Declarative Pipeline if you worked through the previous demonstration. The name of the pipeline created in the previous demonstration is of the form **[dev <usesrname>] health_etl_pipeline_development**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0a36616-44de-4b6a-9408-e3ce20d4c615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cd \"./TODO - Lab DABs Workflow\"\n",
    "databricks bundle run ml_health_etl_workflow -t development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3903f027-e2fc-4d4c-9256-333f8d5d694a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Destroy the bundle for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65654a72-f693-4679-9571-37c1f0befb72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "cd \"./TODO - Lab DABs Workflow\"\n",
    "databricks bundle destroy -t development --auto-approve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9989db1-f94d-4e5e-a74f-b09348ab7564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Staging Bundle Validation, Deployment, and Run\n",
    "\n",
    "Imagine now that you have gone through the process of reviewing your code, analyzed code coverage, etc. and you are ready to now deploy and test within a staging environment. DABs makes this extremely easy by changing and passing a few parameter values. \n",
    "\n",
    "#### Instructions:\n",
    "Using the stage environment (catalog), do the following:\n",
    "1. Explore the stage environment in your **databricks.yml** file and make note of completed changes for stage.\n",
    "\n",
    "1. Run a summary on the bundle.\n",
    "\n",
    "1. Validate the bundle. \n",
    "\n",
    "1. Deploy the bundle. \n",
    "\n",
    "1. Run the bundle. \n",
    "\n",
    "1. Destroy the bundle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "999c89ad-4233-45ea-a3df-483cc3c54415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "cd \"./TODO - Lab DABs Workflow\"\n",
    "databricks bundle summary -t stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f9e6c91-a840-46e7-b501-003fad8f512f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "cd \"./TODO - Lab DABs Workflow\"\n",
    "databricks bundle validate -t stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8179da6e-9a5f-44a6-a50d-eecc4a27a03e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "cd \"./TODO - Lab DABs Workflow\"\n",
    "databricks bundle deploy -t stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1f7080b-ffb5-45ed-99d0-e511a03b6a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "cd \"./TODO - Lab DABs Workflow\"\n",
    "databricks bundle run ml_health_etl_workflow -t stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56aa0bb4-3d39-49e8-837d-984500e73316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Destroy the bundle for stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32f8a612-718b-4098-8b19-0b3971f5ae8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "cd \"./TODO - Lab DABs Workflow\"\n",
    "databricks bundle destroy -t stage --auto-approve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbcb7523-82d9-4399-8bad-13c016a18be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary:\n",
    "\n",
    "In this lab you used a new data asset stored in Unity Catalog to create a new task for your workflow. By understanding how the YAML files are structured, you were able to update your workflow by defining new variables and create a new task.\n",
    "\n",
    "### Next Steps:\n",
    "Try to create your own DAB from scratch using the results from this lab. It's recommended that you incrementally build your workflow one task at a time and making small changes until you are comfortable with understanding the architecture of your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed2f34f-c6f0-4168-996c-67bd9b349ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Lab - Adding ML to Engineering Workflows with DABs",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
