{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "629b66c4-230e-41f7-acd5-30c77503b8de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ac128e2-bb35-424f-8131-5b9a96b95781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creating Anonymized User's Age table\n",
    "\n",
    "In this lesson we'll create a anonymized key for storing potentially sensitive user data.  \n",
    "\n",
    "Our approach in this notebook is fairly straightforward; some industries may require more elaborate de-identification to guarantee privacy.\n",
    "\n",
    "We'll examine design patterns for ensuring PII is stored securely and updated accurately. \n",
    "\n",
    "##### Objectives\n",
    "- Describe the purpose of \"salting\" before hashing\n",
    "- Apply salted hashing to sensitive data(user_id)\n",
    "- Apply tokenization to sensitive data(user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b130f32-918b-4910-b04b-d6e8fc00b6df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A. DAG\n",
    "\n",
    "![demo01_2_anonymization_dag.png](../Includes/images/demo01_2_anonymization_dag.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "201591f7-f932-48eb-b1a9-07d26e6623bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Get the source path for daily user events from Spark configuration\n",
    "daily_user_events_source = spark.conf.get(\"daily_user_events_source\")\n",
    "\n",
    "# Get the catalog name for lookup tables from Spark configuration\n",
    "lookup_catalog = spark.conf.get(\"lookup_catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "692b34ef-9f81-4175-8b54-8d68c860b08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## B. Set up Event User Tables\n",
    "\n",
    "- The **date_lookup** table is used for the **date** and **week_part** association is used to join with the **users_events_raw** data to identify in what **week_part** does the **Date of Birth(DOB)** belongs. _eg: 2020-07-02 = 2020-27_\n",
    "- The **user_events_raw** represents the ingested user event data in JSON, which is later unpacked and filtered to retrieve only user information.\n",
    "- users_bronze: is our focus and will be our source for the ingested user information, where we'll apply **Binning Anonymization** to the **Date of Birth (dob)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ec4dac-ae16-4126-9091-739d155e6741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.table\n",
    "def date_lookup():\n",
    "    # Read the raw date lookup table from the specified catalog\n",
    "    return (spark\n",
    "            .read\n",
    "            .table(f\"{lookup_catalog}.pii_data.date_lookup_raw\")\n",
    "            .select(\"date\", \"week_part\")\n",
    "        )\n",
    "\n",
    "\n",
    "@dp.table(\n",
    "    partition_cols=[\"topic\", \"week_part\"],\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def user_events_raw():\n",
    "    # Read the streaming user events data from the specified source\n",
    "    return (\n",
    "      spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .schema(\"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .load(f\"{daily_user_events_source}\")\n",
    "        .join(\n",
    "          # Join with the date lookup table to get the week part\n",
    "          F.broadcast(dp.read(\"date_lookup\")),  # Broadcasts distributes the lookup table to all executors\n",
    "          F.to_date((F.col(\"timestamp\")/1000).cast(\"timestamp\")) == F.col(\"date\"), \"left\") \n",
    "    )\n",
    "\n",
    "        \n",
    "users_schema = \"user_id LONG, update_type STRING, timestamp FLOAT, dob STRING, sex STRING, gender STRING, first_name STRING, last_name STRING, address STRUCT<street_address: STRING, city: STRING, state: STRING, zip: INT>\"    \n",
    "\n",
    "@dp.table(\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def users_bronze():\n",
    "    # Read the raw user events stream and filter for user info updates\n",
    "    return (\n",
    "        dp.read_stream(\"user_events_raw\") # Reads from user_events_raw\n",
    "          .filter(\"topic = 'user_info'\") # Filters topic with user_info\n",
    "          .select(F.from_json(F.col(\"value\").cast(\"string\"), users_schema).alias(\"v\")) # Unpacks the JSON\n",
    "          .select(\"v.*\") # Select all fields\n",
    "          .select(\n",
    "              # Select and transform the necessary columns\n",
    "              F.col(\"user_id\"),\n",
    "              F.col(\"timestamp\").cast(\"timestamp\").alias(\"updated\"),\n",
    "              F.to_date(\"dob\", \"MM/dd/yyyy\").alias(\"dob\"),\n",
    "              \"sex\", \n",
    "              \"gender\", \n",
    "              \"first_name\", \n",
    "              \"last_name\", \n",
    "              \"address.*\", \n",
    "              \"update_type\"\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bca91a5-42bb-4ab1-9b12-b98ecd944dfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Setup Binning by Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f09cd79b-413b-4cc2-a012-611a46b68277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C.1 Function \"age_bins\"\n",
    "\n",
    "The function `age_bins` takes a date of birth column (**dob_col**) as input.  It calculates the age by finding the difference in months between the current date and the date of birth, then converting it to years.\n",
    "\n",
    "It categorizes the age into bins (e.g., \"under 18\", \"18-25\", etc.) using a series of conditional statements.\n",
    "The resulting age category is returned as a new column named \"age\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e47feae5-8c00-473b-894e-8fbfdd5b44cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def age_bins(dob_col):\n",
    "    age_col = F.floor(F.months_between(F.current_date(), dob_col) / 12).alias(\"age\")\n",
    "    return (\n",
    "        F.when((age_col < 18), \"under 18\")\n",
    "        .when((age_col >= 18) & (age_col < 25), \"18-25\")\n",
    "        .when((age_col >= 25) & (age_col < 35), \"25-35\")\n",
    "        .when((age_col >= 35) & (age_col < 45), \"35-45\")\n",
    "        .when((age_col >= 45) & (age_col < 55), \"45-55\")\n",
    "        .when((age_col >= 55) & (age_col < 65), \"55-65\")\n",
    "        .when((age_col >= 65) & (age_col < 75), \"65-75\")\n",
    "        .when((age_col >= 75) & (age_col < 85), \"75-85\")\n",
    "        .when((age_col >= 85) & (age_col < 95), \"85-95\")\n",
    "        .when((age_col >= 95), \"95+\")\n",
    "        .otherwise(\"invalid age\")\n",
    "        .alias(\"age\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d170f15-4e9d-40fd-b1c4-811ad17af4a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### C.2 Lakeflow Spark Declarative Pipeline Table \"user_age_bins\"\n",
    "\n",
    "It reads data from a source table named **users_bronze**.\n",
    "\n",
    "It selects specific columns: **user_id**, the age category (using the age_bins function on the dob column), gender, city, and state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d16f22-1360-4951-a1cb-d37fd03d6ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.table\n",
    "def user_age_bins():\n",
    "    return (\n",
    "        dp.read(\"users_bronze\")\n",
    "        .select(\"user_id\", age_bins(F.col(\"dob\")), \"gender\", \"city\", \"state\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94702dce-f771-4711-b6ae-195f320a0bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "DP 1.2.2 - Anonymized Users Age",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
